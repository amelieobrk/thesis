\chapter{Discussion}
In this chapter, the presented results will be discussed and interpreted in a larger context. The implications of the findings for real world applications will be considered, as well as the limitations of this thesis and potential future research. 

\section{Addressing Data Leakage and Shortcut Learning}
A central starting point was the unexpectedly strong baseline performance of BERT, which raised questions of potential data leakage and shortcut learning. Addressing this issue formed the methodological foundation for the subsequent analyses.

The near perfect baseline performance (F1 \approx 0.999) observed in initial BERT experiments was identified as a potential symptom of \textbf{data leakage, primarily due to domain and length artifacts between the PJ and PAN12 datasets.} To overcome this, the generation of synthetic non-grooming chats in PJ style with a higher length than the typical PAN12 chats was employed. By introducing synthetic non-grooming PJ chats, the model could no longer rely on source domain as a label proxy and by enforcing stricter train and test splits, chunk-based leakage across datasets was prevented. Also, label smoothing was applied to mitigate overconfidecne in its predictions, as well as an increased droput rate to reduce overfitting on the training data and therfore improve generalization to unseen data. To further analyze the impact of chunk lengths, the improved baseline was tested with a fixed padding on different chunk sizes, revealing that shorter chunks led to a \textbf{slight decrease in performance.} Still, the performance was really high across all three different kind of chunk sizes (512, 250, 150 tokens), indicating that the model could still learn relevant patterns even in smaller text segments. 

\subsubsection{Synthetic Data as a Countermeasure Against Leakage}
It was noticeable, that including synthetic non-grooming Chats only in the test set, a strong drop in performance was visible, highlighting the lack of generalization, as well as a strong domain specific shortcut learning.  \textbf{This showed the risks associated with shortcuts in models, particularly in security applications where robustness is critical. However, questions regarding the generalizability of synthetic negative examples and potential residual artifacts remain open for further investigation.} When integrating the synthetic data in the train set, the performance drop dissapeared, showing that the model could learn to generalize better with the synthetic data. It should be highlighted, that the synthetic data especially affected the precision, since the model achieved a much higher recall already without synthetic data, but the precision was much lower, indicating that the model made more false positive errors. Additionally, the SHAP analyses showed that, with synthetic data, the LIWC feature importance shifted away from formal proxies like length or punctuation towards semantically and psychologically meaningful contrasts. This provides further evidence, that the implemented leakage countermeasures were effective, as the model was forced to rely on psycholinguistic signals rather than dataset specific artifacts. Nevertheless, the use of synthetic data also highlights a limitation. While synthetic non-grooming chats could be generated without ethical concerns, the creation of synthetic grooming conversations with GPT was not feasible due to ethical and policy restrictions. Such data would have represented an even stronger counterbalance against domain leakage, as it could have provided alternative positive examples beyond the PAN12 dataset. Therefore, future work should explore ways to generate or collect ethically sourced positive examples to further enhance model robustness and generalization.

\section{Data Augmentation and Its Limitations in Psycholinguistic Analysis}

To further improve the model's robustness, it could be considered to additonally integrate \textbf{data augmentation techniques} such as paraphrasing or backtranslation in the training data, to increase the diversity of the training data and help the model learn more robust features that are less sensitive to specific wording or phrasing. However, in the present work such methods were deliberately avoided, since \textbf{LIWC features are lexically defined and therefore highly sensitive to textual modifications~\cite{tausczik2010psychological}}. Artificial augmentation (for example synonym replacement, backtranslation) risks shifting the distribution of key categories (for example pronouns, affective terms, sexual language), which would reduce the validity of the following psychometric analyses. Such distortions would likely also affect SHAP explanations, as the method would attribute importance scores based on artificially altered inputs rather than on authentic linguistic patterns, thereby undermining interpretability. 

\section{Baseline Robustness}
Given that all three chunk sizes (150, 250 and 512 tokens) had a consistently strong performance, especially when synthetic data was integrated into the training set, the subsequent feature fusion was conducted with the 512-token mixed configuration. This setup provided the richest conversational context and thereby maximized the coverage of LIWC categories within each chunk, offering the most informative basis for integration with transformer representations and allowing for a more comprehensive analysis of psychometric features with SHAP. The overall strong performance across all settings could be partly attributed to the use of balanced training data, as prior work has shown that transformer-based models such as BERT are highly sensitive to class imbalance and achieve more stable and reliable results under balanced conditions~\cite{henningnlpclassimbalance2023}. Importantly, such balancing was required in this work to enable stable and meaningful SHAP analyses, ensuring that the derived explanations were not dominated by class imbalance effects \parencite{liu2022balancedbackgroundexplanationdata}, \parencite{chen2024interpretable}. 


\section{Cross-Attention Fusion of LIWC and Transformer Representations}

The integration of psycholinguistic features from LIWC into a BERT-based model for online grooming detection has showed clear improvements in model performance and decision stability. The proposed feature-fusion architecture integrated LIWC features with late fusion at layer 6/12 using cross-attention with gating. This design aligns with findings from multimodal fusion research, which suggest that late fusion is particularly effective for heterogeneous modalities and avoids the risks of overloading early linguistic representations~\cite{shankar2022progressivefusion}. Furthermore, attribution studies show that late fusion improves interpretability, as SHAP attributions can more clearly disentangle the contributions of linguistic tokens and auxiliary features~\cite{shapcat2024interpretable}. The performance of the feature-fusion model indicates that integration at layer 6/12 was especially effective in this setting, although the optimal integration depth is likely backbone-specific and requires further investigation. Also, the modular nature of the proposed mechanism makes it transferable to other transformer architectures such as RoBERTa or DeBERTa v3, which are known to be more powerful than BERT~\cite{liu2019roberta}, \cite{he2023debertav3}. However, it should be noted, that these models were trained on a larger corpus, which may include the PAN12 dataset, potentially leading to data leakage if used as a baseline. Therefore, future work should carefully evaluate the use of these models in this context.

\section{Performance Gains from LIWC Integration (Full Set vs. Psychometric Subset)}

The integration of LIWC features improved the F1 score to approximately 0.987 for both the full feature set and the psychometric subset after three epochs, with a \textbf{particularly notable increase in precision, leading to fewer false positives.} As the confusion matrices in figure \ref{fig:ff_confmats_epochs} illustrated, the feature-fusion model led to a more balanced trade-off between precision and recall, with a reduction in false positive errors. This balance of precision and recall is especially relevant in practical applications as it reduces the risk of false alarms, which can be disruptive and lead to unnecessary interventions while keeping the false negative rate low. 

It was shown, that the full LIWC feature set provided slightly stronger effects in the feature-fusion performance and later explainability analysis, while the psychometric subset offered a more streamlined approach with nearly equivalent performance. This has implications for deployment and complexity, suggesting that a reduced feature set may be preferable in resource-constrained environments without significant loss in effectiveness. Still, the psychometric subset allowed an analysis of the most relevant psycholinguistic categories for online grooming detection, especially when combined with SHAP explanations deepening the focus of the analysis beyond only linguistic features. Importantly, once length and domain leakage were mitigated through the use of synthetic data, the analyses revealed that psycholinguistic categories gained weight relative to formal features. In particular, cognition, affect, perception and markers of interpersonal stance (for example tone, clout, authenticity, analytic) consistently emerged among the strongest predictors. Also, SHAP analyses across the full LIWC set and the psychometric subset revealed an overlap in the most influential psychometric categories, especially including cognition and tone. These features consistently emerged as strong indicators to distinguish between grooming and non-grooming behavior regardless of the feature space, suggesting a core set of psychometric markers that carry much of the discriminative signal. At the same time, the full LIWC configuration highlights additional linguistic proxies such as function words and analytic style which might indirectly reflect psychological processes. This indicates that a compact psychometric core could be sufficient for interpretability focused applications, while the extended feature set offers complementary cues that may further strengthen classification in practice.

\section{Stabilizing Effects of LIWC on Model Predictions}

Based on an analysis fo 1000 test samples (Table\ref{tab:liwc_vs_tokens}), it was shown that \textbf{the mean contribution of LIWC features amounts to approximately 9.66\% when using the full LIWC feature set and about 7.41\% for the psychometric subset.} Still, the integration of LIWC features has also been shown to enhance model confidence and stability. By providing additional context and grounding for the model's predictions, LIWC features helped to reduce uncertainty and variability in the decision-making process. This was evidenced by a significant increase in the mean confidence shift (\Delta \mu \approx 0.1543) when LIWC features were included, compared to a much smaller shift (\Delta \mu \approx 0.0219) when only the psychometric subset was used. The low rate of label flips (2.28\% for the full set and 0.13\% for the subset) further underscores the stabilizing effect of LIWC integration, with only changes being conservative reclassifications from grooming to non-grooming. This suggests that LIWC features help to clarify positive borderline cases by providing additional context and reducing ambiguity. Still, the number of label flips was very low, showing that the model's decisions were generally stable and robust. The stabilizing effects may be explained by LIWC’s ability to reduce ambiguity in borderline cases. Prior research has shown that LIWC features:

Further evidence for the stabilizing role of LIWC comes from related work:

\begin{itemize}
    \item LIWC operationalizes psycholinguistic intentions by capturing affective, cognitive and social dimensions of language use \cite{pennebaker2022liwc} and provides more reliable predictors than surface-level text features in personality modeling \cite{farnadi2018user}. These stable markers remain invisible in purely linguistic features and can reduce ambiguity in borderline cases.
    \item In the context of online grooming, LIWC has been shown to clarify behaviors across different stages by highlighting psycholinguistic and discourse patterns \cite{Cano2014} and to distinguish between authentic and deceptive relational intentions \cite{broome2020psycholinguistic}. This could lead to a more contextualized understanding of grooming strategies.
    \item Beyond grooming detection, LIWC has been shown to reduce variance across runs and stabilize predictions when combined with embeddings \cite{mehta2020bottomup}, leading to more consistent outcomes. This aligns with the reduction in label flips and the increased confidence observed in the present work.
\end{itemize}

This suggests, that LIWC contributes to a more nuanced contextualization of conversations, lowering false positives and enhancing both the confidence and stability of model predictions.



\section{LIWC as a Tool for Identifying Grooming and Non-Grooming Mechanisms}

The analysis of LIWC features in grooming and non-grooming chats on a global level, in chunks and using SHAP highlights central psycholinguistic patterns that are closely linked to existing research on cybergrooming. Particularly striking is the high proportion of \textit{tentant}, \textit{future}, \textit{home}, \textit{family} and \textit{affiliation}, \textit{discrep} and \textit{Cognition} in the complete grooming conversations (Figure \ref{fig:liwc_global_analysis}). These findings reflect typical grooming narratives. Cano et al.~\cite{Cano2014} describe that in the trust development phase, references to \textit{home} and \textit{family} dominate, while the approach phase is characterized by planning markers such as \textit{future} and motion semantics. Gupta et al.~\cite{gupta2012characterizingpedophileconversationsinternet} confirm these patterns using LIWC profiles for O’Connell’s six grooming phases, where the categories \textit{family} and \textit{home} serve as markers of risk assessment, \textit{affiliation} indicates relationship building and \textit{sexual terms} represent the sexual phase. The global analyses also highlight the category \textit{discrep} (\textit{would/should/could}), which both Cano et al.~\cite{Cano2014} and Gupta et al.~\cite{gupta2012characterizingpedophileconversationsinternet} describe as a marker for boundary testing and conditioning. The results thus reinforce existing phase and function models and at the same time show that grooming conversations can be identified through a variety of overlapping markers. The same is demonstrated by Chiang and Grant~\cite{chiangandgrant2017online}, who identify 14 rhetorical moves that directly correspond to LIWC categories, including \textit{affiliation} (rapport building), \textit{discrep/tentat} (boundary testing) and \textit{future} (planning).  Lorenzo-Dus and Kinzel~\cite{LorenzoDus2019} and Powell et al.~\cite{powell2021online} further confirm that grooming in practice is not linear but rather “haphazard” and “intermittent, without clear structure.” This assessment explains the global analysis (Figure \ref{fig:liwc_global_analysis}), where all phase markers appear simultaneously, revealing not a strict stage sequence but an overlapping profile.

The chunk-based analysis (Figure \ref{fig:liwc_chunked_analysis}) emphasizes that these markers appear not only at the level of complete conversations but already in short text segments of 512 tokens. This explains why transformer models achieve high detection performance at the chunk level, since local markers such as \textit{(positive)emotion}, \textit{allure}, \textit{polite} \textit{Cognition}, \textit{Social (processes)} and \textit{affiliation} are clearly present there even if the difference is more subtle.

The SHAP analysis (Figure \ref{fig:feature_importance_by_class_combined}) finally provides model-based confirmation of the LIWC features \textit{Tone}, \textit{Authentic}, \textit{Analytic}, \textit{Clout}, \textit{Affect}, \textit{Cognition}, and \textit{Social} for distinguishing grooming from non-grooming conversations. These findings align with research on psycholinguistic markers in online grooming communication. 

Leiva-Bianchi et al.~\cite{leiva2024meta} highlight the high importance of cognitive processes (\textit{cogproc}, \textit{insight}, \textit{discrep}, \textit{tentat}), social markers (\textit{affiliation}, \textit{family}, \textit{friend}), drives (\textit{drives}, \textit{allure}), emotions (\textit{affect}, \textit{emopos}, \textit{emoneg}), as well as sexual and politeness related language for grooming detection. Most of these categories appear among the top 20 SHAP features in the present analysis. Notably, the \textit{Cognition} category consistently dominates across all analytical methods, underscoring its central role in manipulative linguistic strategies. Furthermore Broome et al.~\cite{broome2020psycholinguistic} found that groomers exhibit a characteristic combination of high \textit{Clout} (dominance/self-confidence), positive \textit{Tone}, and moderate \textit{Authenticity}. The SHAP results confirm these three dimensions as highly weighted features, reinforcing their robustness as psycholinguistic markers. Broome et al. further identified \textit{cognitive}, \textit{social} and \textit{present-focus} processes as key communicative patterns, which is also mirrored by the strong SHAP relevance of \textit{Cognition} and \textit{Social}.

Similarly, Black et al.~\cite{black2015linguistic} identified linguistic clusters like \textit{family/home} for risk assessment, \textit{pronouns/affiliation} for exclusivity, \textit{sexual/allure} for sexualization, and \textit{cognition}, \textit{future}, \textit{emotion}, and \textit{polite} for relationship work. These clusters are reflected in the current LIWC analyses. Globally, \textit{family}, \textit{affiliation}, \textit{social}, and \textit{cognition} dominate, at the chunk level, \textit{affiliation} and \textit{cognition} emerge as strong predictors  and in the SHAP values, \textit{affiliation}, \textit{cognition} and \textit{social} appear prominently. In Addition, Cano et al.~\cite{Cano2014} confirm that the LIWC category \textit{discrep} (``would/should/could'') functions as a key linguistic marker of boundary testing and conditioning in the early grooming stages. This finding directly corresponds with the high SHAP relevance of \textit{discrep} observed here. Cano et al. also show that \textit{affect} words, \textit{assent}, and \textit{cognitive mechanisms} are characteristic of the trust development phase—all of which appear among the top SHAP features in the current model. The recent review by Tshimula et al.~\cite{tshimula2024psychologicalprofilingcybersecuritylook} further emphasizes, that psycholinguistic features, particularly linguistic patterns and emotional cues, are effective for cybersecurity applications. They argue that LIWC features capture behavioral indicators of attackers, including increased use of self-focused words, negative language, and cognitive process terms, supporting the prominent SHAP relevance of \textit{Cognition}, \textit{Clout} and \textit{Affect}. Overall, cognitive LIWC features consistently emerge as the strongest predictors across all analyses, additionally aligning with Evans’ dual-process account of reasoning and social cognition~\cite{evans2025corpus}. According to this framework, manipulative communication relies on the interplay between intuitive and analytical processes. Here, tentative and certitude formulations serve to manage uncertainty and reinforce bonding, while social references function as cognitive tools for relational control.

Therefore, the SHAP analysis provides a model based confirmation of the established theoretical dimensions of grooming communication. The convergence between the automatically identified top features and the psycholinguistic markers documented in the literature confirms both the validity of LIWC-based approaches and the robustness of the proposed fusion architecture for practical grooming detection.

\textbf{Overall, LIWC has proven in this work to be a very powerful tool for capturing thematic, linguistic and psychological aspects of grooming conversations.} This became particularly evident through the combination of global, chunk-based and model-driven analyses, which revealed consistent patterns. Notably, the ability of LIWC to detect subtle linguistic cues across different levels of analysis underscores its robustness in the context of grooming detection.

\section{Efficiency potential through reduced feature selection}

The analysis of cumulative feature importance shows that around half of the features already account for around 80\% of the model's predictive power. \textbf{This suggests that there is a certain amount of redundancy in the feature set and that recognition performance could largely be maintained even with a reduced selection of features.} In practice, this implies that models with a reduced LIWC feature set could be designed to be more efficient and resource-friendly without significant losses in model performance. LIWC features like \textit{cognition, social, affect, comm, pronouns i, drives, perception, insight, allure, tentant and positive and negative tone}, as they were among the top rankings in various analyses. These could be tested as a “core subset”, while stylistic features such as \textit{word count, punctuation, or big words} could be used more as a supplement. In addition, a targeted reduction in the number of features could also improve interpretability with SHAP, as the exact calculation of Shapley values is associated with exponential computational effort depending on the number of features \cite{fryer2021shapley}. This would allow a smaller feature subset to enable a more precise and efficient SHAP analysis, as a higher proportion of samples could be analyzed.


\section{Analysis of Misclassifications}

The analysis of misclassifications revealed an asymmetry between false positives and false negatives. False positives showed based on the LIWC features strong proximity to true positives, where false negatives did not exhibit a clear pattern. This finding has strong implications for the precision–recall balance. \textbf{However, it should be noted, that the absolute number of misclassifications in this thesis was very low. Consequently, statistical findings regarding the proximity of false positives and false negatives should be interpreted with caution.}

In the full LIWC feature space, 87\% of false positives were located closer to true positives than to true negatives. Even when restricting the analysis to the psychometric feature subset, the effect persisted, with approximately 66\% of false positives clustering nearer to true positives. Moreover, 44 LIWC features significantly distinguished false positives from true negatives with medium to large effect sizes in the full feature set. These results highlight that false positives are not random errors, but rather conversations that share core LIWC markers with real grooming interactions. Emotional intensity, boundary-testing, affiliation signals and sexual references are typical examples of such overlap. In practice, this includes adult conversations with explicit sexual language, which are highly represented in the PAN12 negative class. The strong similarity of false positives to true positives explains why high precision is difficult to achieve in grooming detection. It is not a failure of the model, but a structural property of the domain, where non-grooming conversations can linguistically resemble grooming exchanges. Consequently, future work should move beyond linguistic analysis and incorporate contextual dimensions like user age, relationship context, platform specific cues and temporal development of conversations. Still, adding features from LIWC has been shown to significantly reduce false positives from the baseline model and improve precision, indicating that these features can help to clarify borderline cases.

By contrast, false negatives displayed no consistent proximity pattern. In the full LIWC feature space, only 46\% clustered closer to true negatives and in the psychometric subset this proportion was 55\%. These values are close to chance level, indicating that false negatives represent borderline cases without a clear proximity to either class. This suggests that the model tends to favor the negative class in ambiguous situations, leading to missed detections of subtle grooming instances. The lack of a consistent LIWC profile for false negatives makes them harder to analyze and address. To improve the classification of false negatives, several methodological approaches can be considered. First, a feature-fusion strategy could integrate additional contextual information such as the relationship between interlocutors (e.g., familiar vs. unfamiliar) or the participants’ age, thereby providing cues that go beyond linguistic content. Second, adjusting the loss function by incorporating $F_{\beta}$ scores could allow the model to prioritize either precision or recall depending on the application context. Third, SHAP-based explainability offers valuable insights by highlighting features where false negatives mimic true negatives. This could inform a targeted re-weighting of categories that are particularly effective in distinguishing false negatives from true negatives, potentially reducing missed detections.  However, to make such re-weighting statistically reliable, a larger pool of false negatives and their associated SHAP values would need to be analyzed, ensuring that selected categories are not driven by individual outliers but represent consistent patterns across cases.


\section{Transparency and Ethical Implications}

When combining LIWC features with transformer representations, the resulting model gains not only a stronger performance but also increases its transparency and interpretability. The use of SHAP explanations allows for a clear attribution of model decisions to specific psycholinguistic features, providing “explainable reasons” for the predictions. This not only increases the interpretability for researchers but also enhances trust among potential end users. LIWC features therefore give black-box models a “psychological grounding”, making their decisions more understandable and justifiable. Nevertheless, the main drawback of SHAP lies in its high computational cost, which currently limits the number of samples that could be analyzed. For a truly robust identification of the most relevant psycholinguistic features, future work will need to scale SHAP analyses to larger datasets. This would allow more statistically reliable differentiation between strong and markers and help to avoid conclusions based on small-sample artifacts. In addition, alternative explainability methods like Integrated Gradients\cite{integratedgradients} and Lime \cite{ribeiro2016lime} could be explored as complementary tools. These approaches may provide different perspectives on feature relevance, reduce computational overhead and together with SHAP yield a more comprehensive interpretability framework.


\section{Broader Limitations and Future Directions}

So far, the discussion has focused on interpreting the main findings and their implications. However, it is also important to show broader limitations of this thesis and outline potential avenues for future research.  

A first set of limitations relates to the data. The PAN12 and PJ datasets are now more than a decade old, meaning that language, slang, and communication styles have naturally evolved since their collection. This temporal gap may limit the applicability of the findings to present grooming conversations. Moreover, it was necessary to extensively preprocess the data to handle slang, abbreviations, and non-standard language. While this preprocessing step was crucial for accurate LIWC feature extraction, it also creates a dependency on the quality on the applied data. In real world applications, where new slang and abbreviations regularly emerge, maintaining such a pipeline would be challenging and may require alternative strategies, such as embedding-based approaches, that can adapt to unseen words in context.  

Another limitation concerns the linguistic scope of the data. Both datasets are written in English, and the LIWC features were derived from an English dictionary. Since LIWC categories are strongly connected to specific lexical items, the findings may not generalize across other language contexts. Still, cybergrooming occurs worldwide across diverse linguistic communities, underscoring the need for multilingual datasets to enable the development of universally applicable detection models.  

Examining the generalizability of the findings further, a core limitation of the Perverted Justice dataset lies in the use of decoy victims. These conversations often involve adult volunteers posing as minors, which produces interaction patterns, linguistic styles, and response behaviors that differ from those of real child victims. As highlighted in prior work \cite{chiangandgrant2017online}, this raises concerns about the authenticity and naturalness of the data, potentially limiting the real world applicability of research findings. Similarly, Broome et al. \cite{broome2020psycholinguistic} emphasize that reliance on decoy victims introduces unnatural conversational dynamics, undermining the general validity of the dataset. Since this thesis relies strongly on linguistic features, some categories like \textit{sexual} or \textit{affiliation} may be over- or underrepresented. For example, decoys may appear more cooperative and responsive, increasing the frequency of affiliation markers, while real victims may exhibit stronger resistance and emotional distress, which would influence the prevalence of affective terms.  

In addition, the dataset used for model training in this thesis was purposely balanced to ensure significant SHAP analyses. While necessary for methodological reasons, this does not reflect the real-world distribution of grooming conversations, which are way less frequent than non-grooming conversations. This unnatural balance may lead to an overestimation of model performance, particularly in terms of precision and recall. Future research should therefore assess models on more realistic and imbalanced datasets to better compare their practical utility. The lack of positive examples also constitutes a broader structural challenge in the field. The lack of publicly available and sufficiently large grooming datasets not only complicates the training of robust models but also increases the risk of domain leakage if training and test data are not strictly separated.  

Finally, certain methodological limitations should be acknowledged . The reliance on BERT-based models with a 512-token limit meant that not all conversations could be fully preserved, even when chunking was applied, leading to potential loss of contextual cues. Furthermore, the fixed training of three epochs without early stopping does not rule out residual overfitting, even if the model was trained with increased dropout and label smoothing to mitigate overconfidence. What limits this concern is, that the relative gains from LIWC fusion over the BERT baseline were consistent across epochs 1–3. Nevertheless, a stricter control would include a small dev set with early stopping, reporting train/dev learning curves and complementing single-split results with group-stratified $k$-fold cross-validation and out-of-domain evaluation. More strategies like learning-curve monitoring, group-stratified cross-validation, and out-of-domain evaluations could also strengthen generalization in future work.  

\textbf{Together, these limitations point towards a need for future research.}

First, future research should explore model architectures that go beyond the 512-token constraint of BERT. Hierarchical architectures and long-context transformer models would enable the processing of entire conversations and all grooming phases rather than truncated segments, thereby preserving important contextual cues. \cite{vogt2021early} Another promising avenue concerns the early detection of grooming. Instead of focusing on full conversations, models could be trained to identify grooming behavior at earlier stages based on LIWC, which would be crucial for timely intervention \cite{vogt2021early}. Such approaches could also investigate which LIWC features are present in different phases of grooming, combining psycholinguistic analysis with machine learning models. However, this would require datasets that explicitly encode conversational phases (for example ChatCoder2, which was developed by McGhee et. al \cite{chatcoder}). 

Secondly, including augmentation techniques like paraphrasing or backtranslation could be analyzed according to the effect on LIWC distributions, explainability analysis and model robustness. 

Also, future research could experiment with different models and fusion strategies. As already mentioned, stronger baselines such as RoBERTa \cite{liu2019roberta} or DeBERTa v3 \cite{he2023debertav3} may outperform BERT, and it would be interesting to test whether integrating LIWC features show improvements in these architectures as well. At the same time, care should be taken since such models are trained on broad corpora that may already contain parts of datasets like PAN12. Beyond transformers, feature fusion with other neural architectures like LSTMs or CNNs could offer deeper insights into how linguistic and psychometric features complement each other with the goal of cybergrooming detection.  

Finally, new research directions could be developed around the role of LIWC features themselves. For example, models could be designed to predict grooming phases based only on LIWC categories, or to evaluate the predictive power of reduced feature sets by training models on only the most informative LIWC categories. Applying the proposed approach directly to the PAN12 dataset would also allow for more direct comparability with previous work and exclude potential data balance effects. However, this would again require an extensive slang handling process to maximize the extraction of relevant LIWC features.


But most important of all:
 
\textbf{Progress in grooming detection will require larger, more diverse, and multilingual datasets, ideally including ethically sourced real victim data. Although this introduces substantial ethical, legal, and privacy challenges, collaborations with law enforcement agencies and child protection organizations may offer options to anonymized datasets that maintain compliance with ethical standards. In parallel, future studies should explore computationally efficient methods for feature selection and dimensionality reduction, architectures capable of modeling longer conversational sequences, and adaptive pipelines that can cope with evolving online language. Addressing these challenges will be key to improving both the robustness and ecological validity of grooming detection models.}  

\begin{comment}



10) Grenzen & Validität (kurz vorwegnehmen, mit Verweis auf Limitations)

Domänen-Shift: Unbekannte Plattformen/Sprachen; Slang-Drift.

Datenetiketten: PJ (Decoy-Chats) ≠ echte Opferdialoge; PAN12-Negativklasse heterogen.

Statistik: Sehr wenige Fehlerfälle im Subset-Setup (n(FN)=40) → geringe Power mancher Tests. 

main

11) Design-Konsequenzen & Future Directions (als Brücke zum Ausblick)

Feintuning auf FN: gezielte Hard-Negative-Mining/Cost-Sensitive Loss für Recall-Stabilität ohne Präzisionseinbruch.

Adaptive Fenster: dynamische Chunking/Hierarchische Modelle für lange Dialoge.

Domänen-Robustheit: Adversarial Training gegen Stil/Längen-Shortcuts; Cross-Platform Evaluation.

Mehr Psycholinguistik: Diskurs-Züge/Pragmatik, Gesprächsakte, turn-taking-Features zusätzlich zu LIWC.

Human-in-the-Loop: Thresholding + Erklärungen für Reviewer-Workflows.


\end{comment}






