\chapter{Discussion}
In this chapter, the presented results will be discussed and interpreted in a larger context. The implications of the findings for real-world applications will be considered, as well as the limitations of the study and potential avenues for future research.


% Interpretation, Relevanz, Limitationen
%Interpretation der Ergebnisse
%Was zeigen SHAP/LIME zu den Features?
%Was bedeutet das für echte Anwendung?
%Limitations
\section{Main Findings and Interpretation}

The main findings of this thesis will be summarized and interpreted in the following sections. The focus will be on the challenges of data leakage and shortcut learning, the strong baseline performance, and the effectiveness of the proposed feature fusion architecture that integrates psycholinguistic features from the LIWC lexicon into a BERT-based model for online grooming detection. 

%%% Hier noch weiter ausbauen


\subsection{Addressing Data Leakage and Shortcut Learning}

The near-perfect baseline performance (F1 \approx 0.999) observed in initial BERT experiments was identified as a potential symptom of \textbf{data leakage, primarily due to domain and length artifacts between the PJ and PAN12 datasets.} To overcome this challenge, the generation of synthetic non-grooming chats in PJ style with a higher length than typical PAN12 chats was employed. By introducing synthetic non-grooming PJ chats, the model could no longer rely on source domain as a label proxy, and by enforcing stricter train/test splits, chunk-based leakage across datasets was prevented. Also, label smoothing was applied to mitigate overconfidecne in its predictions, as well as an increased droput rate to reduce overfitting on the training data and therfore improve generalization to unseen data. To further analyze the impact of chunk lengths, the improved baseline was tested with a fixed padding on different chunk sizes, revealing that shorter chunks led to a \textbf{slight decrease in performance.} Still, the performance was really high across all three different kind of chunk sizes (512, 250, 150 tokens), indicating that the model could still learn relevant patterns even in smaller text segments. 

Also, it was striking, that including synthetic non-grooming Chats only in the test set, a strong drop in performance was visible, highlighting the lack of generalization, as well as a strong domain specific shortcut learning.  \textbf{This highlighted the risks associated with shortcuts in state-of-the-art models, particularly in security applications where robustness is critical. However, questions regarding the generalizability of synthetic negative examples and potential residual artifacts remain open for further investigation.} When integrating the synthetic data in the train set, the performance drop dissapeared, showing that the model could learn to generalize better with the synthetic data. It should be highlighted, that the synthetic data especially affected the precision, since the model achieved a much higher recall already without synthetic data, but the precision was much lower, indicating that the model made more false positive errors. To further improve the model's robustness, it could be considered to additonally integrate \textbf{data augmentation techniques} such as paraphrasing or backtranslation in the training data, to increase the diversity of the training data and help the model learn more robust features that are less sensitive to specific wording or phrasing. However, in the present work such methods were deliberately avoided, since \textbf{LIWC features are lexically defined and therefore highly sensitive to textual modifications~\cite{tausczik2010psychological}}. Artificial augmentation (for example synonym replacement, backtranslation) risks shifting the distribution of key categories (for example pronouns, affective terms, sexual language), which would reduce the validity of subsequent psychometric analyses. Such distortions would likely also affect SHAP explanations, as the method would attribute importance scores based on artificially altered inputs rather than on authentic linguistic patterns, thereby undermining interpretability. 

\subsection{Addressing Baseline Performance}
Given that all three chunk sizes (150, 250, and 512 tokens) had a consistently strong performance, especially when synthetic data was integrated into the training set, the subsequent feature fusion was conducted with the 512-token mixed configuration. This setup provided the richest conversational context and thereby maximized the coverage of LIWC categories within each chunk, offering the most informative basis for integration with transformer representations and allowing for a more comprehensive analysis of psychometric features. The overall strong performance across all settings can be partly attributed to the use of balanced training data, as prior work has shown that transformer-based models such as BERT are highly sensitive to class imbalance and achieve more stable and reliable results under balanced conditions~\cite{henningnlpclassimbalance2023}. Importantly, such balancing was required in this work to enable stable and meaningful SHAP analyses, ensuring that the derived explanations were not dominated by class imbalance effects \parencite{liu2022balancedbackgroundexplanationdata}, \parencite{chen2024interpretable}. Still, overfitting cannot be completely ruled out as an explanation for the overall high performance after three epochs of training, even if the model was trained with increased dropout and label smoothing to mitigate overconfidence. What limits this concern is, that the relative gains from LIWC fusion over the BERT baseline were consistent across epochs 1–3. \textbf{Nevertheless, a stricter control would include a small held-out dev set with early stopping, reporting train/dev learning curves, and complementing single-split results with group-stratified $k$-fold cross-validation and out-of-domain evaluation.}


\subsection{Feature Fusion Architecture (BERT + LIWC via Late Fusion with Cross-Attention)}

The integration of psycholinguistic features from the LIWC lexicon into a BERT-based model for online grooming detection has demonstrated clear improvements in model performance and decision stability. The integration of psycholinguistic features from the LIWC lexicon into a BERT-based model for online grooming detection demonstrated clear improvements in performance and decision stability. The proposed feature-fusion architecture integrated LIWC features via late fusion at layer 6/12 using cross-attention with gating. This design aligns with findings from multimodal fusion research, which suggest that late fusion is particularly effective for heterogeneous modalities and avoids the risks of overloading early linguistic representations~\cite{shankar2022progressivefusion}. Furthermore, attribution studies show that late fusion improves interpretability, as SHAP attributions can more clearly disentangle the contributions of linguistic tokens and auxiliary features~\cite{shapcat2024interpretable}. The performance of the feature-fusion model indicates that integration at layer 6/12 was especially effective in this setting, although the optimal integration depth is likely backbone-specific and requires further investigation. Also, the modular nature of the proposed mechanism makes it transferable to other transformer architectures such as RoBERTa or DeBERTa v3, which are known to be more powerful than BERT~\cite{liu2019roberta}, \cite{he2023debertav3}. However, it should be noted that these models were trained on a much broader corpus, which may include the PAN12 dataset, potentially leading to data leakage if used as a baseline. Therefore, future work should carefully evaluate the use of these models in this context.

\subsection{Performance Gains from LIWC Integration (Full Set vs. Psychometric Subset)}

The integration of LIWC features consistently improved the F1 score to approximately 0.987 for both the full feature set and the psychometric subset after three epochs, with a \textbf{particularly notable increase in precision, leading to fewer false positives.} As the confusion matrices in figure \ref{fig:ff_confmats_epochs} illustrated, the feature-fusion model led to a more balanced trade-off between precision and recall, with a significant reduction in false positive errors. This balance of precision and recall is especially relevant in practical applications as it reduces the risk of false alarms, which can be disruptive and lead to unnecessary interventions while keeping the false negative rate low. The improved precision is particularly valuable in clinical and forensic contexts, where the consequences of false positives can be significant, including reputational damage and increased workload for human reviewers.

It was shown, that the full LIWC feature set provided slightly stronger effects in the feature-fusion performance and later explainability analysis, while the psychometric subset offered a more streamlined approach with nearly equivalent performance. This has implications for deployment and complexity, suggesting that a reduced feature set may be preferable in resource-constrained environments without significant loss in effectiveness. Still, the psychometric subset allowed an analysis of the most relevant psycholinguistic categories for online grooming detection, especially when combined with SHAP explanations deepening the focus of the analysis beyond only linguistic features. Still, SHAP analyses across both the full LIWC set and the psychometric subset reveal a substantial overlap in the most influential psychometric categories, especially including cognition and social words. These features consistently emerge as strong indicators to distinguish between grooming and non-grooming behavior regardless of the feature space, suggesting a core set of psychometric markers that carry much of the discriminative signal. At the same time, the full LIWC configuration highlights additional linguistic proxies such as function words, analytic style, and word count, which indirectly reflect psychological processes. This indicates that a compact psychometric core could be sufficient for interpretability-focused applications, while the extended feature set offers complementary cues that may further strengthen classification in practice.

\subsection{Model Confidence and Stability}

The integration of LIWC features has also been shown to enhance model confidence and stability. By providing additional context and grounding for the model's predictions, LIWC features help to reduce uncertainty and variability in the decision-making process. This was evidenced by a significant increase in the median confidence shift ($\\Delta$ \mu  \approx 0.1543) when LIWC features were included, compared to a much smaller shift ($\\Delta$ \mu  \approx 0.0219) when only the psychometric subset was used. The low rate of label flips (2.28\% for the full set and 0.13\% for the subset) further underscores the stabilizing effect of LIWC integration, with most changes being conservative reclassifications from grooming to non-grooming. This suggests that LIWC features help to clarify positive borderline cases by providing additional context and reducing ambiguity. Still, the number of label flips was very low, indicating that the model's decisions were generally stable and robust. This stability is particularly important in high-stakes applications where consistent and reliable predictions are crucial.

\subsection{LIWC-Features in Grooming and Non-Grooming Chats}

The analysis of the LIWC-Features in grooming and non-grooming on a global, chunk level and shap basis revealed several key insights into the psycholinguistic patterns associated with online grooming behavior and draw a direct line with the existing literature based on a psycholinguistic analysis of cybergrooming. Most striking is the high proportion of \textit{future}, \textit{home}, \textit{family}, and \textit{affiliation} in the PJ grooming conversations (Figure \ref{fig:liwc_global_analysis}). These findings reflect typical grooming narratives, as described by Cano et al.~\cite{Cano2014}. In the trust development phase, categories such as \textit{future} and \textit{home} frequently occur, while the approach phase is marked by planning and frame semantics (``arriving'', ``visiting''). Gupta et al.~\cite{gupta2012characterizingpedophileconversationsinternet} confirm these patterns, identifying \textit{family} as a marker of risk assessment (queries about parents and environment) and \textit{time/motion} as characteristic for the conclusion/approach stage. Thus, the analysis of complete conversations (Figure \ref{fig:liwc_global_analysis}) replicate established stage models of grooming.  

A particularly strong signal lies in the category \textit{discrep} (\textit{would/should/could}), which is also elevated in the global PJ grooming conversations analysis of the psychometric subset (Figure \ref{fig:liwc_global_analysis}, right). This aligns with the top predictors of the approach phase reported by Cano et al.~\cite{Cano2014}, where \textit{discrep} and \textit{funct} serve as tools for boundary testing and conditioning. Slightly increased values for \textit{tentat} further indicate strategies of relationship maintenance and polite, indirect approaches, as also observed by Broome et al.~\cite{broome2020psycholinguistic} in LIWC profiles of convicted offenders.

Beyond these psychometric dimensions, the results also highlight differences in thematic scope. Grooming conversations contain more social, relationship-oriented markers (\textit{pronouns}, \textit{function words}, \textit{affiliation}), while the PAN12 non-grooming data are more strongly characterized by sexualized but non-grooming conversations (Figure \ref{fig:liwc_global_analysis}, Figure \ref{fig:liwc_chunked_analysis}). This is consistent with Gupta et al.~\cite{gupta2012characterizingpedophileconversationsinternet}, who list \textit{social} as a marker of friendship forming and \textit{sexual} as an indicator of the sexual phase. Broome et al.~\cite{broome2020psycholinguistic} also found that groomers primarily act in a relationship- and present-focused manner, while explicit sexual language is comparatively rare. This explains why sexualized markers are diluted in the global measures due to thematic diversity, but become more salient on the chunk level (Figure \ref{fig:liwc_chunked_analysis}).

The chunk based analysis underlines, that these markers are not only detectable globally, but also already apparent in short text segments of 512 tokens (Figure \ref{sec:chunk_based_liwc_analysis}). This complements the work of Cano et al.~\cite{Cano2014}, who classify grooming processes at the line level according to stage-specific markers, and explains why models such as BERT can achieve high detection performance even at the chunk level. While global analyses emphasize length and diversity, local analyses highlight specific psycholinguistic patterns (sexuality, allure, cognition, social processes) that become especially evident in LIWC-based profiles.  

Methodologically, it must be noted that formal features such as word count and sentence length represent strong confounders, as also emphasized by Broome et al.~\cite{broome2020psycholinguistic}. Without synthetic data, the model’s decision shifts more strongly towards such formal proxies, whereas with synthetic data, semantic and psycholinguistic contrasts become more pronounced (Figure \ref{fig:feature_importance_by_class_combined}). This supports the argument of An et al.~\cite{an2025cybergrooming}, who state that data quality and balance are crucial to avoid artificial correlations and to make the underlying psycholinguistic processes visible.  




When looking at the SHAP explanations (Figure \ref{fig:feature_importance_by_class_combined}), it is evident that the direction of effect of several features changes when synthetic data is included. This could be explained by the fact that SHAP values are \textbf{always model- and dataset-specific}. Through the integration of synthetic data, the underlying feature distributions and their correlations with the target variable shift, leading to cases where features associated with grooming in real data appear more frequently in non-grooming contexts when synthetic data are included. Moreover, the sample-based calculation of SHAP values contributes to this effect. Since the analysis was performed on a randomly selected subset of 256 instances, \textbf{sampling variance can result in changes in the direction of individual features.} These issues are amplified when synthetic data are used, as their distributions may differ from real data, causing further adjustments of model parameters and the relative attribution of features. Overall, this highlights the sensitivity of explainable AI methods to data augmentation and sampling, and demonstrates that interpretations of SHAP directions should \textbf{always be made cautiously and in the context of the specific training and evaluation data.}




\begin{comment}
\paragraph{Einordnung und Konsequenzen.} von Signed Direction der Features:


5) Confidence-Shift & Label-Flip Analyse (Stabilität der Entscheidungen)

Befund: LIWC erhöht die Modell-Sicherheit klar (Median-∆p≈0.236 bei allen LIWC vs. ≈0.031 Subset); Label-Flips selten (2.28% bzw. 0.13%) und praktisch einseitig von Grooming→Non-Grooming (konservativer).

Diskutiere: Warum zusätzliche Psycholinguistik Zweifel an “grenzwertigen” Positiven ausräumt; Risiken (verpasste True Positives?) vs. Nutzen (weniger Fehlalarme).

6) LIWC-Analysen (global & chunkbasiert)

Global vs. lokal: Auch auf Chunk-Ebene bleiben Kernsignale sichtbar (Sexual-Termini, kognitive/soziale Prozesse) → unterstützt dein Chunk-Training. 

main

Diskussionspunkt: Was bedeutet das für frühe Erkennung (kleine Textfenster)? Potenzial für Sliding-Window/Online-Setup.

7) Fehlklassifikationen (Volcano/Proximity/SHAP)

Starkes Muster: False Positives ähneln True Positives deutlich mehr als True Negatives (≈87% näher an TP im vollen LIWC-Raum; hochsignifikant). Das erklärt, warum Präzision schwer ist. 

main

Psychometrisches Subset: Muster bleibt, aber schwächer (≈66% FP näher an TP; signifikant).

False Negatives: Kein konsistentes “TN-ähnlich”-Profil; heterogener, schwerer erklärbar → Ansatzpunkte für Modell-/Feature-Design. 

main

Was heißt das inhaltlich?: Grooming-nahe, aber unschuldige Chats (z. B. explicit talk unter Erwachsenen) teilen psycholinguistische Marker mit echten Grooming-Fällen → Bedrohung für precision-first Systeme; Bedarf an Kontextmetadaten/Pragmatik.

8) Rolle der Synthetic Data in den Erklärungen

Deine Auswertung: Mit Synthetic Data verschiebt sich die kumulative LIWC-Bedeutung Richtung “semantisch-psychologischer Kontraste” statt formaler Proxies (Länge, Interpunktion). Diskutiere als Evidenz, dass deine Leakage-Gegenmaßnahmen wirken. 

main

9) Praxis-Implikationen & Ethik

Operate-Mode: Für Moderation/Forensik ist hohe Precision wertvoll (weniger Rufschädigung/Arbeitslast). Deine Fusion unterstützt das (mehr konservative Re-Einstufungen).

Transparenz: SHAP-Attributionen der LIWC-Merkmale als “prüfbare Gründe” für Entscheidungen → erhöht Akzeptanz. 

main

Ethik/Policy: Keine synthetischen Grooming-Chats erzeugt → verantwortungsvoll; dennoch Bias-Risiken (englischsprachig, PJ/PAN12-Sourcing).

10) Grenzen & Validität (kurz vorwegnehmen, mit Verweis auf Limitations)

Domänen-Shift: Unbekannte Plattformen/Sprachen; Slang-Drift.

Datenetiketten: PJ (Decoy-Chats) ≠ echte Opferdialoge; PAN12-Negativklasse heterogen.

Statistik: Sehr wenige Fehlerfälle im Subset-Setup (n(FN)=40) → geringe Power mancher Tests. 

main

11) Design-Konsequenzen & Future Directions (als Brücke zum Ausblick)

Feintuning auf FN: gezielte Hard-Negative-Mining/Cost-Sensitive Loss für Recall-Stabilität ohne Präzisionseinbruch.

Adaptive Fenster: dynamische Chunking/Hierarchische Modelle für lange Dialoge.

Domänen-Robustheit: Adversarial Training gegen Stil/Längen-Shortcuts; Cross-Platform Evaluation.

Mehr Psycholinguistik: Diskurs-Züge/Pragmatik, Gesprächsakte, turn-taking-Features zusätzlich zu LIWC.

Human-in-the-Loop: Thresholding + Erklärungen für Reviewer-Workflows.


\end{comment}






