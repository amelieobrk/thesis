\chapter{Discussion}
In this chapter, the presented results will be discussed and interpreted in a larger context. The implications of the findings for real-world applications will be considered, as well as the limitations of the study and potential avenues for future research.


% Interpretation, Relevanz, Limitationen
%Interpretation der Ergebnisse
%Was zeigen SHAP/LIME zu den Features?
%Was bedeutet das für echte Anwendung?
%Limitations
\section{Main Findings and Interpretation}

The main findings of this thesis will be summarized and interpreted in the following sections. The focus will be on the challenges of data leakage and shortcut learning, the strong baseline performance and the effectiveness of the proposed feature fusion architecture that integrates psycholinguistic features from the LIWC lexicon into a BERT-based model for online grooming detection. 

%%% Hier noch weiter ausbauen


\subsection{Addressing Data Leakage and Shortcut Learning}
A central starting point was the unexpectedly strong baseline performance of BERT, which raised questions of potential data leakage and shortcut learning. Addressing this issue formed the methodological foundation for the subsequent analyses.

The near-perfect baseline performance (F1 \approx 0.999) observed in initial BERT experiments was identified as a potential symptom of \textbf{data leakage, primarily due to domain and length artifacts between the PJ and PAN12 datasets.} To overcome this challenge, the generation of synthetic non-grooming chats in PJ style with a higher length than typical PAN12 chats was employed. By introducing synthetic non-grooming PJ chats, the model could no longer rely on source domain as a label proxy and by enforcing stricter train/test splits, chunk-based leakage across datasets was prevented. Also, label smoothing was applied to mitigate overconfidecne in its predictions, as well as an increased droput rate to reduce overfitting on the training data and therfore improve generalization to unseen data. To further analyze the impact of chunk lengths, the improved baseline was tested with a fixed padding on different chunk sizes, revealing that shorter chunks led to a \textbf{slight decrease in performance.} Still, the performance was really high across all three different kind of chunk sizes (512, 250, 150 tokens), indicating that the model could still learn relevant patterns even in smaller text segments. 


\subsubsection{Synthetic Data as a Countermeasure Against Leakage}
It was striking, that including synthetic non-grooming Chats only in the test set, a strong drop in performance was visible, highlighting the lack of generalization, as well as a strong domain specific shortcut learning.  \textbf{This showed the risks associated with shortcuts in state-of-the-art models, particularly in security applications where robustness is critical. However, questions regarding the generalizability of synthetic negative examples and potential residual artifacts remain open for further investigation.} When integrating the synthetic data in the train set, the performance drop dissapeared, showing that the model could learn to generalize better with the synthetic data. It should be highlighted, that the synthetic data especially affected the precision, since the model achieved a much higher recall already without synthetic data, but the precision was much lower, indicating that the model made more false positive errors. Additionally, the SHAP analyses showed that, with synthetic data, the LIWC feature importance shifted away from formal proxies such as length or punctuation towards semantically and psychologically meaningful contrasts. This provides further evidence that the implemented leakage countermeasures were effective, as the model was forced to rely on genuine psycholinguistic signals rather than dataset-specific artifacts. Nevertheless, the use of synthetic data also highlights a limitation. While synthetic non-grooming chats could be generated without ethical concerns, the creation of synthetic grooming conversations with GPT was not feasible due to ethical and policy restrictions. Such data would have represented an even stronger counterbalance against domain leakage, as it could have provided alternative positive examples beyond the PAN12 dataset. 

\subsection{Data Augmentation and Its Limitations in Psycholinguistic Analysis}

To further improve the model's robustness, it could be considered to additonally integrate \textbf{data augmentation techniques} such as paraphrasing or backtranslation in the training data, to increase the diversity of the training data and help the model learn more robust features that are less sensitive to specific wording or phrasing. However, in the present work such methods were deliberately avoided, since \textbf{LIWC features are lexically defined and therefore highly sensitive to textual modifications~\cite{tausczik2010psychological}}. Artificial augmentation (for example synonym replacement, backtranslation) risks shifting the distribution of key categories (for example pronouns, affective terms, sexual language), which would reduce the validity of the following psychometric analyses. Such distortions would likely also affect SHAP explanations, as the method would attribute importance scores based on artificially altered inputs rather than on authentic linguistic patterns, thereby undermining interpretability. 

\subsection{Baseline Robustness and Overfitting Considerations}
Given that all three chunk sizes (150, 250 and 512 tokens) had a consistently strong performance, especially when synthetic data was integrated into the training set, the subsequent feature fusion was conducted with the 512-token mixed configuration. This setup provided the richest conversational context and thereby maximized the coverage of LIWC categories within each chunk, offering the most informative basis for integration with transformer representations and allowing for a more comprehensive analysis of psychometric features. The overall strong performance across all settings could be partly attributed to the use of balanced training data, as prior work has shown that transformer-based models such as BERT are highly sensitive to class imbalance and achieve more stable and reliable results under balanced conditions~\cite{henningnlpclassimbalance2023}. Importantly, such balancing was required in this work to enable stable and meaningful SHAP analyses, ensuring that the derived explanations were not dominated by class imbalance effects \parencite{liu2022balancedbackgroundexplanationdata}, \parencite{chen2024interpretable}. Still, overfitting cannot be completely ruled out as an explanation for the overall high performance after three epochs of training, even if the model was trained with increased dropout and label smoothing to mitigate overconfidence. What limits this concern is, that the relative gains from LIWC fusion over the BERT baseline were consistent across epochs 1–3. \textbf{Nevertheless, a stricter control would include a small held-out dev set with early stopping, reporting train/dev learning curves and complementing single-split results with group-stratified $k$-fold cross-validation and out-of-domain evaluation.}


\subsection{Cross-Attention Fusion of LIWC and Transformer Representations}

The integration of psycholinguistic features from LIWC into a BERT-based model for online grooming detection has demonstrated clear improvements in model performance and decision stability. The proposed feature-fusion architecture integrated LIWC features via late fusion at layer 6/12 using cross-attention with gating. This design aligns with findings from multimodal fusion research, which suggest that late fusion is particularly effective for heterogeneous modalities and avoids the risks of overloading early linguistic representations~\cite{shankar2022progressivefusion}. Furthermore, attribution studies show that late fusion improves interpretability, as SHAP attributions can more clearly disentangle the contributions of linguistic tokens and auxiliary features~\cite{shapcat2024interpretable}. The performance of the feature-fusion model indicates that integration at layer 6/12 was especially effective in this setting, although the optimal integration depth is likely backbone-specific and requires further investigation. Also, the modular nature of the proposed mechanism makes it transferable to other transformer architectures such as RoBERTa or DeBERTa v3, which are known to be more powerful than BERT~\cite{liu2019roberta}, \cite{he2023debertav3}. However, it should be noted that these models were trained on a much broader corpus, which may include the PAN12 dataset, potentially leading to data leakage if used as a baseline. Therefore, future work should carefully evaluate the use of these models in this context.

\subsection{Performance Gains from LIWC Integration (Full Set vs. Psychometric Subset)}

The integration of LIWC features consistently improved the F1 score to approximately 0.987 for both the full feature set and the psychometric subset after three epochs, with a \textbf{particularly notable increase in precision, leading to fewer false positives.} As the confusion matrices in figure \ref{fig:ff_confmats_epochs} illustrated, the feature-fusion model led to a more balanced trade-off between precision and recall, with a significant reduction in false positive errors. This balance of precision and recall is especially relevant in practical applications as it reduces the risk of false alarms, which can be disruptive and lead to unnecessary interventions while keeping the false negative rate low. 

It was shown, that the full LIWC feature set provided slightly stronger effects in the feature-fusion performance and later explainability analysis, while the psychometric subset offered a more streamlined approach with nearly equivalent performance. This has implications for deployment and complexity, suggesting that a reduced feature set may be preferable in resource-constrained environments without significant loss in effectiveness. Still, the psychometric subset allowed an analysis of the most relevant psycholinguistic categories for online grooming detection, especially when combined with SHAP explanations deepening the focus of the analysis beyond only linguistic features. Importantly, once length and domain leakage were mitigated through the use of synthetic data, the analyses revealed that psycholinguistic categories gained substantially in weight relative to formal features. In particular, cognition, social processes, affect, perception and markers of interpersonal stance (for example tone, clout, authenticity) consistently emerged among the strongest predictors, whereas purely formal variables such as word count or function words receded in importance.Also, SHAP analyses across both the full LIWC set and the psychometric subset revealed an overlap in the most influential psychometric categories, especially including cognition and social words. These features consistently emerged as strong indicators to distinguish between grooming and non-grooming behavior regardless of the feature space, suggesting a core set of psychometric markers that carry much of the discriminative signal. At the same time, the full LIWC configuration highlights additional linguistic proxies such as function words, analytic style and word count, which indirectly reflect psychological processes. This indicates that a compact psychometric core could be sufficient for interpretability-focused applications, while the extended feature set offers complementary cues that may further strengthen classification in practice.

\subsection{Stabilizing Effects of LIWC on Model Predictions}

The integration of LIWC features has also been shown to enhance model confidence and stability. By providing additional context and grounding for the model's predictions, LIWC features help to reduce uncertainty and variability in the decision-making process. This was evidenced by a significant increase in the median confidence shift (\Delta \mu \approx 0.1543) when LIWC features were included, compared to a much smaller shift (\Delta \mu \approx 0.0219) when only the psychometric subset was used. The low rate of label flips (2.28\% for the full set and 0.13\% for the subset) further underscores the stabilizing effect of LIWC integration, with only changes being conservative reclassifications from grooming to non-grooming. This suggests that LIWC features help to clarify positive borderline cases by providing additional context and reducing ambiguity. Still, the number of label flips was very low, showing that the model's decisions were generally stable and robust. The stabilizing effects may be explained by LIWC’s ability to reduce ambiguity in borderline cases. Prior research has shown that LIWC features:

Further evidence for the stabilizing role of LIWC comes from related work:

\begin{itemize}
    \item LIWC operationalizes psycholinguistic intentions by capturing affective, cognitive and social dimensions of language use \cite{pennebaker2022liwc} and provides more reliable predictors than surface-level text features in personality modeling \cite{farnadi2018user}. These stable markers remain invisible in purely linguistic features and can reduce ambiguity in borderline cases.
    \item In the context of online grooming, LIWC has been shown to clarify behaviors across different stages by highlighting psycholinguistic and discourse patterns \cite{Cano2014} and to distinguish between authentic and deceptive relational intentions \cite{broome2020psycholinguistic}. This could lead to a more contextualized understanding of grooming strategies.
    \item Beyond grooming detection, LIWC has been shown to reduce variance across runs and stabilize predictions when combined with embeddings \cite{mehta2020bottomup}, leading to more consistent outcomes. This aligns with the reduction in label flips and the increased confidence observed in the present work.
\end{itemize}

This suggests, that LIWC contributes to a more nuanced contextualization of conversations, lowering false positives and enhancing both the confidence and stability of model predictions.



\subsection{LIWC as a Tool for Identifying Grooming and Non-Grooming Mechanisms}

The analysis of LIWC features in grooming and non-grooming chats on a global level, in chunks and using SHAP highlights central psycholinguistic patterns that are closely linked to existing research on cybergrooming. Particularly striking is the high proportion of \textit{future}, \textit{home}, \textit{family} and \textit{affiliation} in the complete grooming conversations (Figure \ref{fig:liwc_global_analysis}). These findings reflect typical grooming narratives. Cano et al.~\cite{Cano2014} describe that in the trust development phase, references to \textit{home} and \textit{family} dominate, while the approach phase is characterized by planning markers such as \textit{future} and motion semantics. Gupta et al.~\cite{gupta2012characterizingpedophileconversationsinternet} confirm these patterns using LIWC profiles for O’Connell’s six grooming phases, where the categories \textit{family} and \textit{home} serve as markers of risk assessment, \textit{affiliation} indicates relationship building and \textit{sexual terms} represent the sexual phase. Broome et al.~\cite{broome2020psycholinguistic} add that groomers often use \textit{tentat} and \textit{polite} in their language profiles to build trust and shift boundaries indirectly, which also corresponds to the results in Figures \ref{fig:liwc_global_analysis}, \ref{fig:liwc_chunked_analysis} and \ref{fig:feature_importance_by_class_combined}.

The global analyses also highlight the category \textit{discrep} (\textit{would/should/could}), which both Cano et al.~\cite{Cano2014} and Gupta et al.~\cite{gupta2012characterizingpedophileconversationsinternet} describe as a marker for boundary testing and conditioning. The results thus reinforce existing phase and function models and at the same time show that grooming conversations can be identified through a variety of overlapping markers. The same is demonstrated by Chiang and Grant~\cite{chiangandgrant2017online}, who identify 14 rhetorical moves that directly correspond to LIWC categories, including \textit{affiliation} (rapport building), \textit{discrep/tentat} (boundary testing) and \textit{future} (planning). Lorenzo-Dus and Kinzel~\cite{LorenzoDus2019} and Powell et al.~\cite{powell2021online} further confirm that grooming in practice is not linear but rather “haphazard” and “intermittent, without clear structure.” This assessment explains the global analysis (Figure \ref{fig:liwc_global_analysis}), where all phase markers appear simultaneously, revealing not a strict stage sequence but an overlapping profile.

The chunk-based analysis (Figure \ref{fig:liwc_chunked_analysis}) emphasizes that these markers appear not only at the level of complete conversations but already in short text segments of 512 tokens. Kloess et al.~\cite{kloess2014online} document that sexualized topics are introduced “relatively early” and “within minutes”, while Webster et al.~\cite{webster2021european} emphasize that grooming strategies are “rapidly employed” and not developed gradually. This is reflected in the fact that especially short grooming segments contain high proportions of \textit{sexual}, which are lost in global averages. This also explains why transformer models achieve high detection performance at the chunk level, since local markers such as \textit{sexual}, \textit{allure}, \textit{cognition} and \textit{affiliation} are clearly present there.

The SHAP analysis (Figure \ref{fig:feature_importance_by_class_combined}) finally provides model-based confirmation of these dimensions. Leiva-Bianchi et al.~\cite{leiva2024meta} show in a meta-analysis that cognitive processes (\textit{cogproc, insight, discrep, tentat}), social markers (\textit{affiliation, family, friend}), drives (\textit{drives, allure}), emotions (\textit{affect, emopos, emoneg}), sexual language and politeness markers are particularly decisive for grooming detection. Exactly these categories also appear among our Top-20 SHAP features. Evans~\cite{evans2025corpus} additionally assigns these dimensions a functional role. Groomers and children use language to perform particular actions. Thus, \textit{future} markers become planning instruments, \textit{tentative/certitude} formulations serve as tools for uncertainty and commitment, \textit{social references} act as means of relationship building and \textit{cognition} becomes the central tool of manipulative control.  

Black et al.~\cite{black2015linguistic} also show that groomers rely on a clear set of markers such as \textit{family/home} for risk assessment, \textit{pronouns/affiliation} for exclusivity, \textit{sexual/allure} for sexualization, as well as \textit{cognition, future, emotion, polite} for relationship work. These clusters are mirrored in the LIWC analyses conducted here. Globally, the categories \textit{family, home, future, affiliation, social, cognition, discrep} dominate, at chunk level, \textit{sexual, allure, discrep, cognition, affiliation} emerge and in the SHAP values, \textit{discrep, family, affiliation, cognition, emotion, polite, social} are most visible. \textbf{Striking is, that especially cognitive and social LIWC features consistently appear as the strongest markers across all analyses.}

\textbf{Overall, LIWC has proven in this work to be a very powerful tool for capturing thematic, linguistic and psychological aspects of grooming conversations.} This became particularly evident through the combination of global, chunk-based and model-driven analyses, which revealed consistent patterns. Other studies also underline this methodological strength. Tshimula et al.~\cite{tshimula2024psychologicalprofilingcybersecuritylook} demonstrate in the field of cyber-threat profiling that “psycholinguistic features, such as linguistic patterns and emotional cues” remain reliably identifiable even in short text segments. Thus, LIWC proves to be not only a strong tool for grooming analysis but also a robust method for uncovering psycholinguistic mechanisms more broadly. 


\subsection{Efficiency potential through reduced feature selection}

The analysis of cumulative feature importance shows that around half of the features already account for around 80\% of the model's predictive power. \textbf{This suggests that there is a certain amount of redundancy in the feature set and that recognition performance could largely be maintained even with a reduced selection of features.} In practice, this implies that models with a reduced LIWC feature set could be designed to be more efficient and resource-friendly without significant losses in model performance. LIWC features like \textit{cognition, social, affect, comm, pronouns i, drives, perception, insight, allure, tentant and positive and negative tone}, as they were among the top rankings in various analyses. These could be tested as a “core subset”, while stylistic features such as \textit{word count, punctuation, or big words} could be used more as a supplement. In addition, a targeted reduction in the number of features could also improve interpretability with SHAP, as the exact calculation of Shapley values is associated with exponential computational effort depending on the number of features \cite{fryer2021shapley}. This would allow a smaller feature subset to enable a more precise and efficient SHAP analysis, as a higher proportion of samples could be analyzed.


\subsection{Analysis of Misclassifications}

The analysis of misclassifications revealed an asymmetry between false positives and false negatives. False positives showed based on the LIWC features strong proximity to true positives, where false negatives did not exhibit a clear pattern. This finding has strong implications for the precision–recall balance. \textbf{However, it should be noted, that the absolute number of misclassifications in this thesis was very low. Consequently, statistical findings regarding the proximity of false positives and false negatives should be interpreted with caution.}

In the full LIWC feature space, 87\% of false positives were located closer to true positives than to true negatives. Even when restricting the analysis to the psychometric feature subset, the effect persisted, with approximately 66\% of false positives clustering nearer to true positives. Moreover, 44 LIWC features significantly distinguished false positives from true negatives with medium to large effect sizes in the full feature set. These results highlight that false positives are not random errors, but rather conversations that share core LIWC markers with real grooming interactions. Emotional intensity, boundary-testing, affiliation signals and sexual references are typical examples of such overlap. In practice, this includes adult conversations with explicit sexual language, which are highly represented in the PAN12 negative class. The strong similarity of false positives to true positives explains why high precision is difficult to achieve in grooming detection. It is not a failure of the model, but a structural property of the domain, where non-grooming conversations can linguistically resemble grooming exchanges. Consequently, future work should move beyond linguistic analysis and incorporate contextual dimensions like user age, relationship context, platform specific cues and temporal development of conversations. Still, adding features from LIWC has been shown to significantly reduce false positives from the baseline model and improve precision, indicating that these features can help to clarify borderline cases.

By contrast, false negatives displayed no consistent proximity pattern. In the full LIWC feature space, only 46\% clustered closer to true negatives and in the psychometric subset this proportion was 55\%. These values are close to chance level, indicating that false negatives represent borderline cases without a clear proximity to either class. This suggests that the model tends to favor the negative class in ambiguous situations, leading to missed detections of subtle grooming instances. The lack of a consistent LIWC profile for false negatives makes them harder to analyze and address. To improve the classification of false negatives, several methodological approaches can be considered. First, a feature-fusion strategy could integrate additional contextual information such as the relationship between interlocutors (e.g., familiar vs. unfamiliar) or the participants’ age, thereby providing cues that go beyond linguistic content. Second, adjusting the loss function by incorporating $F_{\beta}$ scores could allow the model to prioritize either precision or recall depending on the application context. Third, SHAP-based explainability offers valuable insights by highlighting features where false negatives mimic true negatives. This could inform a targeted re-weighting of categories that are particularly effective in distinguishing false negatives from true negatives, potentially reducing missed detections.  However, to make such re-weighting statistically reliable, a larger pool of false negatives and their associated SHAP values would need to be analyzed, ensuring that selected categories are not driven by individual outliers but represent consistent patterns across cases.

\subsection{Transparency and Ethical Implications}

When combining LIWC features with transformer representations, the resulting model gains not only a stronger performance but also increases its transparency and interpretability. The use of SHAP explanations allows for a clear attribution of model decisions to specific psycholinguistic features, providing “explainable reasons” for the predictions. This not only increases the interpretability for researchers but also enhances trust among potential end users. LIWC features therefore give black-box models a “psychological grounding”, making their decisions more understandable and justifiable. Nevertheless, the main drawback of SHAP lies in its high computational cost, which currently limits the number of samples that could be analyzed. For a truly robust identification of the most relevant psycholinguistic features, future work will need to scale SHAP analyses to larger datasets. This would allow more statistically reliable differentiation between strong and markers and help to avoid conclusions based on small-sample artifacts. In addition, alternative explainability methods like Integrated Gradients\cite{integratedgradients} and Lime \cite{ribeiro2016lime} could be explored as complementary tools. These approaches may provide different perspectives on feature relevance, reduce computational overhead and together with SHAP yield a more comprehensive interpretability framework.


\begin{comment}



10) Grenzen & Validität (kurz vorwegnehmen, mit Verweis auf Limitations)

Domänen-Shift: Unbekannte Plattformen/Sprachen; Slang-Drift.

Datenetiketten: PJ (Decoy-Chats) ≠ echte Opferdialoge; PAN12-Negativklasse heterogen.

Statistik: Sehr wenige Fehlerfälle im Subset-Setup (n(FN)=40) → geringe Power mancher Tests. 

main

11) Design-Konsequenzen & Future Directions (als Brücke zum Ausblick)

Feintuning auf FN: gezielte Hard-Negative-Mining/Cost-Sensitive Loss für Recall-Stabilität ohne Präzisionseinbruch.

Adaptive Fenster: dynamische Chunking/Hierarchische Modelle für lange Dialoge.

Domänen-Robustheit: Adversarial Training gegen Stil/Längen-Shortcuts; Cross-Platform Evaluation.

Mehr Psycholinguistik: Diskurs-Züge/Pragmatik, Gesprächsakte, turn-taking-Features zusätzlich zu LIWC.

Human-in-the-Loop: Thresholding + Erklärungen für Reviewer-Workflows.


\end{comment}






