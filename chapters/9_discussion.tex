\chapter{Discussion}
In this chapter, the presented results will be discussed and interpreted in a larger context. The implications of the findings for real world applications will be considered, as well as the limitations of this thesis and potential future research. 

\section{Addressing Data Leakage and Shortcut Learning}
A central starting point was the unexpectedly strong baseline performance of BERT, which raised questions of potential data leakage and shortcut learning. Addressing this issue formed the methodological foundation for the subsequent analyses.

The near perfect baseline performance (F1 \approx 0.999) observed in initial BERT experiments was identified as a potential symptom of \textbf{data leakage, primarily due to domain and length artifacts between the PJ and PAN12 datasets.} To overcome this, the generation of synthetic non-grooming chats in PJ style with a higher length than the typical PAN12 chats was employed. By introducing synthetic non-grooming PJ chats, the model could no longer rely on source domain as a label proxy and by enforcing stricter train and test splits, chunk-based leakage across datasets was prevented. Also, label smoothing was applied to mitigate overconfidecne in its predictions, as well as an increased droput rate to reduce overfitting on the training data and therfore improve generalization to unseen data. To further analyze the impact of chunk lengths, the improved baseline was tested with a fixed padding on different chunk sizes, revealing that shorter chunks led to a \textbf{slight decrease in performance.} Still, the performance was really high across all three different kind of chunk sizes (512, 250, 150 tokens), indicating that the model could still learn relevant patterns even in smaller text segments. 

\subsubsection{Synthetic Data as a Countermeasure Against Leakage}
It was noticeable, that including synthetic non-grooming Chats only in the test set, a strong drop in performance was visible, highlighting the lack of generalization, as well as a strong domain specific shortcut learning.  \textbf{This showed the risks associated with shortcuts in models, particularly in security applications where robustness is critical. However, questions regarding the generalizability of synthetic negative examples and potential residual artifacts remain open for further investigation.} When integrating the synthetic data in the train set, the performance drop dissapeared, showing that the model could learn to generalize better with the synthetic data. It should be highlighted, that the synthetic data especially affected the precision, since the model achieved a much higher recall already without synthetic data, but the precision was much lower, indicating that the model made more false positive errors. Additionally, the SHAP analyses showed that, with synthetic data, the LIWC feature importance shifted away from formal proxies like length or punctuation towards semantically and psychologically meaningful contrasts. This provides further evidence, that the implemented leakage countermeasures were effective, as the model was forced to rely on psycholinguistic signals rather than dataset specific artifacts. Nevertheless, the use of synthetic data also highlights a limitation. While synthetic non-grooming chats could be generated without ethical concerns, the creation of synthetic grooming conversations with GPT was not feasible due to ethical and policy restrictions. Such data would have represented an even stronger counterbalance against domain leakage, as it could have provided alternative positive examples beyond the PAN12 dataset. Therefore, future work should explore ways to generate or collect ethically sourced positive examples to further enhance model robustness and generalization.

\section{Data Augmentation and Its Limitations in Psycholinguistic Analysis}

To further improve the model's robustness, it could be considered to additonally integrate \textbf{data augmentation techniques} such as paraphrasing or backtranslation in the training data, to increase the diversity of the training data and help the model learn more robust features that are less sensitive to specific wording or phrasing. However, in the present work such methods were deliberately avoided, since \textbf{LIWC features are lexically defined and therefore highly sensitive to textual modifications~\cite{tausczik2010psychological}}. Artificial augmentation (for example synonym replacement, backtranslation) risks shifting the distribution of key categories (for example pronouns, affective terms, sexual language), which would reduce the validity of the following psychometric analyses. Such distortions would likely also affect SHAP explanations, as the method would attribute importance scores based on artificially altered inputs rather than on authentic linguistic patterns, thereby undermining interpretability. 

\section{Baseline Robustness}
Given that all three chunk sizes (150, 250 and 512 tokens) had a consistently strong performance, especially when synthetic data was integrated into the training set, the subsequent feature fusion was conducted with the 512-token mixed configuration. This setup provided the richest conversational context and thereby maximized the coverage of LIWC categories within each chunk, offering the most informative basis for integration with transformer representations and allowing for a more comprehensive analysis of psychometric features with SHAP. The overall strong performance across all settings could be partly attributed to the use of balanced training data, as prior work has shown that transformer-based models such as BERT are highly sensitive to class imbalance and achieve more stable and reliable results under balanced conditions~\cite{henningnlpclassimbalance2023}. Importantly, such balancing was required in this work to enable stable and meaningful SHAP analyses, ensuring that the derived explanations were not dominated by class imbalance effects \parencite{liu2022balancedbackgroundexplanationdata}, \parencite{chen2024interpretable}. 


\section{Cross-Attention Fusion of LIWC and Transformer Representations}

The integration of psycholinguistic features from LIWC into a BERT-based model for online grooming detection has showed clear improvements in model performance and decision stability. The proposed feature-fusion architecture integrated LIWC features with late fusion at layer 6/12 using cross-attention with gating. This design aligns with findings from multimodal fusion research, which suggest that late fusion is particularly effective for heterogeneous modalities and avoids the risks of overloading early linguistic representations~\cite{shankar2022progressivefusion}. Furthermore, attribution studies show that late fusion improves interpretability, as SHAP attributions can more clearly disentangle the contributions of linguistic tokens and auxiliary features~\cite{shapcat2024interpretable}. The performance of the feature-fusion model indicates that integration at layer 6/12 was especially effective in this setting, although the optimal integration depth is likely backbone-specific and requires further investigation. Also, the modular nature of the proposed mechanism makes it transferable to other transformer architectures such as RoBERTa or DeBERTa v3, which are known to be more powerful than BERT~\cite{liu2019roberta}, \cite{he2023debertav3}. However, it should be noted, that these models were trained on a larger corpus, which may include the PAN12 dataset, potentially leading to data leakage if used as a baseline. Therefore, future work should carefully evaluate the use of these models in this context.

\section{Performance Gains from LIWC Integration (Full Set vs. Psychometric Subset)}

The integration of LIWC features improved the F1 score to approximately 0.987 for both the full feature set and the psychometric subset after three epochs, with a \textbf{particularly notable increase in precision, leading to fewer false positives.} As the confusion matrices in figure \ref{fig:ff_confmats_epochs} illustrated, the feature-fusion model led to a more balanced trade-off between precision and recall, with a reduction in false positive errors. This balance of precision and recall is especially relevant in practical applications as it reduces the risk of false alarms, which can be disruptive and lead to unnecessary interventions while keeping the false negative rate low. 

It was shown, that the full LIWC feature set provided slightly stronger effects in the feature-fusion performance and later explainability analysis, while the psychometric subset offered a more streamlined approach with nearly equivalent performance. This has implications for deployment and complexity, suggesting that a reduced feature set may be preferable in resource-constrained environments without significant loss in effectiveness. Still, the psychometric subset allowed an analysis of the most relevant psycholinguistic categories for online grooming detection, especially when combined with SHAP explanations deepening the focus of the analysis beyond only linguistic features. Importantly, once length and domain leakage were mitigated through the use of synthetic data, the analyses revealed that psycholinguistic categories gained weight relative to formal features. In particular, cognition, affect, perception and markers of interpersonal stance (for example tone, clout, authenticity, analytic) consistently emerged among the strongest predictors. Also, SHAP analyses across the full LIWC set and the psychometric subset revealed an overlap in the most influential psychometric categories, especially including cognition and tone. These features consistently emerged as strong indicators to distinguish between grooming and non-grooming behavior regardless of the feature space, suggesting a core set of psychometric markers that carry much of the discriminative signal. At the same time, the full LIWC configuration highlights additional linguistic proxies such as function words and analytic style which might indirectly reflect psychological processes. This indicates that a compact psychometric core could be sufficient for interpretability focused applications, while the extended feature set offers complementary cues that may further strengthen classification in practice.

\section{Stabilizing Effects of LIWC on Model Predictions}

Based on an analysis fo 1000 test samples (Table\ref{tab:liwc_vs_tokens}), it was shown that \textbf{the mean contribution of LIWC features amounts to approximately 9.66\% when using the full LIWC feature set and about 7.41\% for the psychometric subset.} Still, the integration of LIWC features has also been shown to enhance model confidence and stability. By providing additional context and grounding for the model's predictions, LIWC features helped to reduce uncertainty and variability in the decision-making process. This was evidenced by a significant increase in the mean confidence shift (\Delta \mu \approx 0.1543) when LIWC features were included, compared to a much smaller shift (\Delta \mu \approx 0.0219) when only the psychometric subset was used. The low rate of label flips (2.28\% for the full set and 0.13\% for the subset) further underscores the stabilizing effect of LIWC integration, with only changes being conservative reclassifications from grooming to non-grooming. This suggests that LIWC features help to clarify positive borderline cases by providing additional context and reducing ambiguity. Still, the number of label flips was very low, showing that the model's decisions were generally stable and robust. The stabilizing effects may be explained by LIWC’s ability to reduce ambiguity in borderline cases. Prior research has shown that LIWC features:

Further evidence for the stabilizing role of LIWC comes from related work:

\begin{itemize}
    \item LIWC operationalizes psycholinguistic intentions by capturing affective, cognitive and social dimensions of language use \cite{pennebaker2022liwc} and provides more reliable predictors than surface-level text features in personality modeling \cite{farnadi2018user}. These stable markers remain invisible in purely linguistic features and can reduce ambiguity in borderline cases.
    \item In the context of online grooming, LIWC has been shown to clarify behaviors across different stages by highlighting psycholinguistic and discourse patterns \cite{Cano2014} and to distinguish between authentic and deceptive relational intentions \cite{broome2020psycholinguistic}. This could lead to a more contextualized understanding of grooming strategies.
    \item Beyond grooming detection, LIWC has been shown to reduce variance across runs and stabilize predictions when combined with embeddings \cite{mehta2020bottomup}, leading to more consistent outcomes. This aligns with the reduction in label flips and the increased confidence observed in the present work.
\end{itemize}

This suggests, that LIWC contributes to a more nuanced contextualization of conversations, lowering false positives and enhancing both the confidence and stability of model predictions.



\section{LIWC as a Tool for Identifying Grooming and Non-Grooming Mechanisms}

The analysis of LIWC features in grooming and non-grooming chats on a global level, in chunks and using SHAP highlights central psycholinguistic patterns that are closely linked to existing research on cybergrooming. Particularly striking is the high proportion of \textit{Cognition} (together with\textit{discrepancy} and \textit{tentative language}), \textit{future focus}, \textit{home}, \textit{family} and \textit{affiliation} in the complete grooming conversations (Figure \ref{fig:liwc_global_analysis}). These findings reflect typical grooming narratives. Cano et al.~\cite{Cano2014} describe that in the trust development phase, references to \textit{home} and \textit{family} dominate, while the approach phase is characterized by planning markers such as \textit{future} and motion semantics. Gupta et al.~\cite{gupta2012characterizingpedophileconversationsinternet} confirm these patterns using LIWC profiles for O’Connell’s six grooming phases, where the categories \textit{family} and \textit{home} serve as markers of risk assessment, \textit{affiliation} indicates relationship building and \textit{sexual terms} represent the sexual phase. The global analyses also highlight the category \textit{discrepancy} (\textit{would/should/could}), which both Cano et al.~\cite{Cano2014} and Gupta et al.~\cite{gupta2012characterizingpedophileconversationsinternet} describe as a marker for boundary testing and conditioning. The results therefore reinforce existing phase and function models and at the same time show, that grooming conversations can be identified through a variety of overlapping markers. The same is demonstrated by Chiang and Grant~\cite{chiangandgrant2017online}, who identify 14 rhetorical moves that directly correspond to LIWC categories, including \textit{affiliation} (rapport building), \textit{discrepancy/tentative language} (boundary testing) and \textit{future} (planning).  Lorenzo-Dus and Kinzel~\cite{LorenzoDus2019} and Powell et al.~\cite{powell2021online} further confirm that grooming in practice is not linear but rather “haphazard” and “intermittent, without clear structure.” This assessment explains the global analysis (Figure \ref{fig:liwc_global_analysis}), where all phase markers appear simultaneously, revealing not a strict stage sequence but an overlapping profile.

The chunk-based analysis (Figure \ref{fig:liwc_chunked_analysis}) highlight that these markers appear not only at the level of complete conversations but already in short text segments of 512 tokens. This explains why transformer models achieve high detection performance at the chunk level, since local markers such as \textit{(positive)emotion}, \textit{allure}, \textit{Cognition} with \textit{cognitive processes}, \textit{Drives} with \textit{affiliation} and \textit{Social (behavior)} with \textit{politeness} are clearly present there even if the difference is more subtle.

The SHAP analysis (Figure \ref{fig:feature_importance_by_class_combined}) provides model-based confirmation that the LIWC features \textit{Tone}, \textit{Authentic}, \textit{Analytic}, and \textit{Clout} representing overall summary variables as well as broader linguistic domains like  \textit{Affect}, \textit{Cognition} and \textit{Social processes} (each encompassing several subcategories), are key predictors for distinguishing grooming from non-grooming conversations. These findings also align with prior research on psycholinguistic markers in online grooming communication. Across the literature, \textbf{cognitive and social processes} emerge as central indicators of manipulative intent. Leiva-Bianchi et al.~\cite{leiva2024meta} showed the relevance of \textit{cognitive processes} (\textit{cogproc}, \textit{insight}, \textit{discrep}, \textit{tentat}), \textit{social markers} (\textit{affiliation}, \textit{family}, \textit{friend}), \textit{drives} (\textit{drives}, \textit{allure}), and \textit{emotional expressions} (\textit{affect}, \textit{emopos}, \textit{emoneg}), as well as sexual and politeness related language for grooming detection. Most of these categories appear among the top 20 SHAP features in the present analysis, with \textit{Cognition} dominating across all analytical methods underscoring its important role in manipulative strategies. This cognitive focus is further complemented by \textbf{interpersonal and affective markers}. Broome et al.~\cite{broome2020psycholinguistic} found that groomers typically exhibit a characteristic combination of high \textit{Clout} (dominance/self-confidence), \textit{positive tone}, and moderate \textit{Authenticity}. The SHAP results confirm these three dimensions as highly weighted features, reinforcing their robustness as psycholinguistic markers. Broome et al. also identified \textit{cognitive}, \textit{social}, and \textit{present-focus} processes as key communicative patterns which are also mirrored here by the strong SHAP relevance of the features \textit{Cognition} and \textit{Social}. A similar constellation of linguistic signals was identified by \textbf{Black et al.}~\cite{black2015linguistic}, who grouped relevant markers into thematic clusters containing \textit{family/home} for risk assessment, \textit{pronouns/affiliation} for exclusivity, \textit{sexual/allure} for sexualization, and \textit{Cognition}, \textit{future}, \textit{emotion} and \textit{polite} for relationship work. These clusters are reflected in the present LIWC analyses where globally, \textit{family}, \textit{affiliation}, \textit{Social}, and \textit{Cognition} dominate and at the chunk level, \textit{affiliation} and \textit{Cognition} emerge as strong predictors while within the SHAP values, \textit{affiliation}, \textit{Cognition}, and \textit{Social} appear prominently. 

\textbf{In addition to these category level observations, a closer inspection of the underlying LIWC subfeatures clarifies which linguistic dimensions most strongly drive the observed distinctions.} The summary variables \textit{Analytic}, \textit{Clout}, \textit{Authentic}, and \textit{Tone} showed the highest model relevance, confirming that overall stylistic tone and psychological stance are strong global indicators. Within the broader domains, \textbf{Cognition} exhibited the most consistent influence across analyses, primarily through \textit{cognitive processes}, \textit{insight}, \textit{cause}, \textit{discrepancy}, and \textit{tentative language}. These subfeatures capture reasoning and uncertainty expressions that differentiate communicative patterns between both classes. \textbf{Affect} was driven by \textit{positive tone}, \textit{positive emotion}, and \textit{swear}, while \textbf{Drives} were characterized by \textit{affiliation}. The \textbf{Social processes} domain was mainly shaped by \textit{social behavior}, \textit{politeness}, and \textit{female} references, indicating differences in interpersonal framing. Additionally, the features \textbf{allure} and \textbf{sexual} stood out as individually relevant predictors, highlighting thematic contrasts beyond the aggregated categories. Together, these findings demonstrate that linguistic mechanisms contribute to LIWC’s discriminative capacity across analytical levels.

Beyond these patterns, \textbf{specific linguistic mechanisms} further show the model’s alignment with prior theoretical accounts. Cano et al.~\cite{Cano2014} demonstrated that the LIWC category \textit{discrepancy} ("would/should/could") serves as a key marker of boundary testing and conditioning in early grooming stages, which again corresponds directly to the high SHAP relevance of \textit{discrep} observed in this thesis. The \textbf{cybersecurity relevance} of such psycholinguistic dimensions has also been emphasized in recent work. Tshimula et al.~\cite{tshimula2024psychologicalprofilingcybersecuritylook} highlight that linguistic and emotional cues are behavioral indicators of attackers, as LIWC captures increased self-focus, negative language, and cognitive process terms. These findings support the prominent SHAP relevance of \textit{Cognition}, \textit{Clout}, and \textit{Affect}. Collectively, cognitive LIWC features consistently emerge as the most powerful predictors across all analyses. Finally, these converging results also align with \textbf{Evans’ dual-process account of reasoning and social cognition}~\cite{evans2025corpus}. According to this framework, manipulative communication relies on the interplay between intuitive and analytical processes where tentative or certain formulations help manage uncertainty and foster bonding, while social references function as cognitive tools for relational control.

Therefore, the SHAP analysis provides a confirmation of the established theoretical dimensions of grooming communication. \textbf{Overall, LIWC has proven in this work to be a very powerful tool for capturing thematic, linguistic and psychological aspects of grooming conversations.} This became evident through the combination of global, chunk-based and model-driven analyses, which revealed consistent patterns. Notably, the ability of LIWC to detect subtle linguistic cues across different levels of analysis underscores its robustness in the context of grooming detection.

\section{Efficiency potential through reduced feature selection}

The analysis of cumulative feature importance shows that around half of the features already account more than 80\% of the model's predictive power. \textbf{This suggests that there is a certain amount of redundancy in the feature set and that recognition performance could be maintained even with a reduced selection of features.} In practice, this suggests that models could focus on a smaller set of high-level LIWC dimensions that have shown the strongest discriminative power across analyses. Instead of including the full range of fine-grained categories, a model could rely primarily on overarching psycholinguistic domains such as \textit{Tone}, \textit{Authenticity}, \textit{Analytic Thinking}, and \textit{Clout}, as well as the fundamental linguistic–psychological categories \textit{Affect}, \textit{Cognition}, and \textit{Social Processes}. These dimensions have consistently emerged as the most informative and interpretable predictors. More linguistic features, such as \textit{Word Count}, \textit{Punctuation}, or \textit{Big Words}, could instead serve as supplementary indicators rather than core features. In addition, a targeted reduction in the number of features could also improve interpretability with SHAP, as the exact calculation of Shapley values is associated with exponential computational effort depending on the number of features \cite{fryer2021shapley}.

\section{Analysis of Misclassifications}

The analysis of misclassifications revealed an asymmetry between false positives and false negatives. False positives showed based on the LIWC features strong proximity to true positives, where false negatives did not exhibit a clear pattern. This finding has strong implications for the precision–recall balance. \textbf{However, it should be noted, that the absolute number of misclassifications in this thesis was very low. Consequently, statistical findings regarding the proximity of false positives and false negatives should be interpreted with caution.}

In the full LIWC feature space, 87\% of false positives were located closer to true positives than to true negatives. Even when restricting the analysis to the psychometric feature subset, the effect persisted, with approximately 66\% of false positives clustering nearer to true positives. Moreover, 44 LIWC features significantly distinguished false positives from true negatives with medium to large effect sizes in the full feature set. These results highlight, that false positives are not random errors, but rather conversations that share core LIWC markers with real grooming interactions. The strong similarity of false positives to true positives explains, why high precision is difficult to achieve in grooming detection. It is not a failure of the model, but a property of the domain, where non-grooming conversations can linguistically be more similar to grooming conversations. Consequently, future work should move beyond linguistic analysis and could incorporate contextual dimensions like user age, relationship context, platform specific cues and temporal development of conversations. Still, adding features from LIWC has been shown to reduce false positives from the baseline model and improve precision, indicating that these features can help to clarify borderline cases.

By contrast, false negatives displayed no consistent proximity pattern. In the full LIWC feature space, only 46\% clustered closer to true negatives and in the psychometric subset this proportion was 55\%. These values are close to chance level, indicating that false negatives represent borderline cases without a clear proximity to either class. This suggests, that the model tends to favor the negative class in ambiguous situations, leading to missed detections of subtle grooming instances. The lack of a consistent LIWC profile for false negatives makes them harder to analyze and address. To improve the classification of false negatives, some methodological approaches can be considered. First, a feature-fusion strategy could integrate additional contextual information like the relationship between interlocutors (for example familiar vs. unfamiliar) or the age of the authors, providing information that go beyond linguistic content. Second, adjusting the loss function by incorporating $F_{\beta}$ scores could allow the model to prioritize either precision or recall depending on the application context. Third, SHAP based explainability offers valuable insights by highlighting features where false negatives mimic true negatives. This could inform a targeted re-weighting of categories, that are particularly effective in distinguishing false negatives from true negatives, potentially reducing missed detections.  

\section{Transparency and Ethical Implications}

When combining LIWC features with transformer representations, the resulting model gains not only a stronger performance but also increases its transparency and interpretability. The use of SHAP explanations allows for a clear attribution of model decisions to specific psycholinguistic features, providing “explainable reasons” for the predictions. This not only increases the interpretability for researchers but also enhances trust among potential end users. LIWC features therefore give black-box models a “psychological grounding”, making their decisions more understandable and justifiable. Nevertheless, the main drawback of SHAP lies in its high computational cost, which limits the number of samples that can be analyzed. Therefore, alternative explainability methods like Integrated Gradients\cite{integratedgradients} and Lime \cite{ribeiro2016lime} could be explored as complementary tools. These approaches may provide different perspectives on feature relevance, reduce computational overhead and together with SHAP give a more comprehensive explainability framework.

\section{Broader Limitations and Future Directions}

So far, the discussion has focused on interpreting the main findings and their implications. However, it is also important to show broader limitations of this thesis and outline potential avenues for future research.  

A first set of limitations relates to the data. The PAN12 and PJ datasets are now more than a decade old, meaning that language, slang, and communication styles have naturally evolved since their collection. This temporal gap may limit the applicability of the findings to present grooming conversations. Moreover, it was necessary to extensively preprocess the data to handle slang. While this preprocessing step was crucial for accurate LIWC feature extraction, it also creates a dependency on the quality on the applied data. In real world applications, where new slang and abbreviations regularly emerge, maintaining such a pipeline would be challenging and may require alternative strategies, such as embedding-based approaches, that can adapt to unseen words in context.  

Another limitation concerns the linguistic scope of the data. Both datasets are written in English, and the LIWC features were derived from an English dictionary. Since LIWC categories are strongly connected to specific lexical items, the findings may not generalize across other language contexts. Still, cybergrooming occurs worldwide across diverse linguistic communities, underscoring the need for multilingual datasets to enable the development of universally applicable detection models.  

Examining the generalizability of the findings further, a core limitation of the Perverted Justice dataset lies in the use of decoy victims. These conversations often involve adult volunteers posing as minors, which produces interaction patterns, linguistic styles, and response behaviors that differ from those of real child victims. As highlighted in prior work \cite{chiangandgrant2017online}, this raises concerns about the authenticity and naturalness of the data, potentially limiting the real world applicability of research findings. Similarly, Broome et al. \cite{broome2020psycholinguistic} emphasize that reliance on decoy victims introduces unnatural conversational dynamics, undermining the general validity of the dataset. Since this thesis relies strongly on linguistic features, some categories like \textit{sexual} or \textit{affiliation} may be over or underrepresented in the PJ data. For example, decoys may appear more cooperative and responsive, increasing the frequency of affiliation markers, while real victims may exhibit stronger resistance and emotional distress, which would influence the prevalence of affective terms.  

In addition, the dataset used for model training in this thesis was purposely balanced to ensure SHAP analyses. While necessary for methodological reasons, this does not reflect the real world distribution of grooming conversations, which are way less frequent than non-grooming conversations. This unnatural balance may lead to an overestimation of model performance, particularly in terms of precision and recall. Future research should therefore assess models on more realistic and imbalanced datasets to better compare their practical utility. The lack of positive examples also constitutes a broader structural challenge in the field. The lack of publicly available and sufficiently large grooming datasets not only complicates the training of robust models but also increases the risk of domain leakage if training and test data are not strictly separated.  

Finally, certain methodological limitations should be acknowledged . The reliance on BERT-based models with a 512-token limit meant that not all conversations could be fully preserved, even when chunking was applied, leading to potential loss of contextual cues. Furthermore, the fixed training of three epochs without early stopping does not rule out residual overfitting, even if the model was trained with increased dropout and label smoothing to mitigate overconfidence. What limits this concern is, that the relative gains from LIWC fusion over the BERT baseline were consistent across epochs 1–3. Nevertheless, a stricter control would include a small dev set with early stopping, reporting train/dev learning curves and complementing single-split results with group-stratified $k$-fold cross-validation and out-of-domain evaluation. More strategies like learning-curve monitoring, group-stratified cross-validation, and out-of-domain evaluations could also strengthen generalization in future work.  

\textbf{Together, these limitations point towards a need for future research.}

First, future research should explore model architectures that go beyond the 512-token constraint of BERT. Hierarchical architectures and long-context transformer models would enable the processing of entire conversations and all grooming phases rather than truncated segments, thereby preserving important contextual cues. \cite{vogt2021early} Another promising avenue concerns the early detection of grooming. Instead of focusing on full conversations, models could be trained to identify grooming behavior at earlier stages based on LIWC, which would be crucial for timely intervention \cite{vogt2021early}. Such approaches could also investigate which LIWC features are present in different phases of grooming, combining psycholinguistic analysis with machine learning models. However, this would require datasets that explicitly encode conversational phases (for example ChatCoder2, which was developed by McGhee et. al \cite{chatcoder}). 

Secondly, including augmentation techniques like paraphrasing or backtranslation could be analyzed according to the effect on LIWC distributions, explainability analysis and model robustness. 

Also, future research could experiment with different models and fusion strategies. As already mentioned, stronger baselines such as RoBERTa \cite{liu2019roberta} or DeBERTa v3 \cite{he2023debertav3} may outperform BERT, and it would be interesting to test whether integrating LIWC features show improvements in these architectures as well. At the same time, care should be taken since such models are trained on broad corpora that may already contain parts of datasets like PAN12. Beyond transformers, feature fusion with other neural architectures like LSTMs or CNNs could offer deeper insights into how linguistic and psychometric features complement each other with the goal of cybergrooming detection.  

Finally, new research directions could be developed around the role of LIWC features themselves. For example, models could be designed to predict grooming phases based only on LIWC categories, or to evaluate the predictive power of reduced feature sets by training models on only the most informative LIWC categories. Applying the proposed approach directly to the PAN12 dataset would also allow for more direct comparability with previous work and exclude potential data balance effects. However, this would again require an extensive slang handling process to maximize the extraction of relevant LIWC features.


But most important of all:
 
\textbf{Progress in grooming detection will require larger, more diverse, and multilingual datasets, ideally including ethically sourced real victim data. Although this introduces substantial ethical, legal, and privacy challenges, collaborations with law enforcement agencies and child protection organizations may offer options to anonymized datasets that maintain compliance with ethical standards. In parallel, future studies should explore computationally efficient methods for feature selection and dimensionality reduction, architectures capable of modeling longer conversational sequences, and adaptive pipelines that can cope with evolving online language. Addressing these challenges will be key to improving both the robustness and ecological validity of grooming detection models.}  








