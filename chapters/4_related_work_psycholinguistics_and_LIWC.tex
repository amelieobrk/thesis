\section{Linguistic Inquiry and Word Count (LIWC)}
Linguistic Inquiry and Word Count (LIWC) was first developed by James W. Pennebaker and colleagues as a theory driven tool for the analysis of written and spoken language~\cite{tausczik2010psychological}. 
The main goal of LIWC is to extract language into psychological states, attitudes, and personality traits based on a predefined dictionary that maps words to meaningful linguistic categories. LIWC analyzes the text by counting the frequecy of words in these categories, providing a quantitative representation of the language used. This allows drawing conclusions about the psychological and social dimensions of the written text.
The construction of the LIWC dictionary and category system followed an iterative process. Initially, relevant words were selected based on psychological theory and expert evaluation. Then, these words were revised, adapted and psychometrically validated through continuous testing and updated over time in response to new research findings and computational methods~\cite{tausczik2010psychological}. The LIWC dictionary has been continuously refined with its most recent version released in 2022~\cite{pennebaker2022liwc}.

%geändert

\subsection{LIWC-22 Categories}

The categories in LIWC-22 are hierarchically organized and can be divided into three main levels~\cite{pennebaker2022liwc,tausczik2010psychological}. Together, they capture the \textit{content} and \textit{style} of language, enabling a comprehensive psycholinguistic analysis.

\begin{itemize}
    \item \textbf{Dictionary-based categories:} These represent the core lexical dimensions of LIWC and are directly based on dictionary word counts. Each category corresponds to a psychological, social, or linguistic construct. Broader \textbf{macro categories} (for example \textit{Cognitive processes}) comprise multiple \textbf{subcategories} (for example \textit{insight}, \textit{causation}, \textit{discrepancy}, \textit{tentative}, \textit{certitude}). A single word may belong to several categories simultaneously. For example, the word “cry” appears in \textit{negative emotion} and \textit{sadness}. The main lexical domains in LIWC-22 include:
        \begin{itemize}
            \item \textit{Linguistic dimensions:} Syntactic and functional aspect like personal pronouns (for example first-person singular/plural), articles, auxiliary verbs, verb tenses, negations, and other grammatical markers.
            \item \textit{Psychological processes:} Affective, cognitive, and social dimensions, such as:
                \begin{itemize}
                    \item \textit{Affective processes:} Positive and negative emotions, anger, anxiety, sadness, and swearing.
                    \item \textit{Cognitive processes:} Insight, causation, discrepancy, tentativeness, certainty, differentiation, memory, and analytical thinking.
                    \item \textit{Social processes:} Family, friends, gender references, group membership, communication, politeness, moralization, and prosocial behavior.
                \end{itemize}
            \item \textit{Personal concerns:} Work, money, religion, home, leisure, sexuality, politics, ethnicity, and technology.
            \item \textit{Biological and physical processes:} Health, illness, wellness, mental states, body, food, substances, and death.
            \item \textit{Cultural phenomena:} politics, ethnicity, netspeak, conversational markers, and emojis.
        \end{itemize}

    \item \textbf{Summary variables:} In addition to the dictionary-based categories, LIWC-22 includes four \textbf{algorithmically derived summary variables} containing \textit{Analytic thinking}, \textit{Clout}, \textit{Authenticity} and \textit{Emotional tone}. These variables are not based on word frequencies but are computed through regression based algorithms trained on large corpora and psychometric data~\cite{pennebaker2022liwc}. They capture higher order stylistic and structural features of language like reasoning style (\textit{Analytic}), perceived social confidence (\textit{Clout}), honesty and self-disclosure (\textit{Authenticity}) and emotional tone (\textit{Tone}). While lexical categories describe \textit{what} people communicate, these summary variables characterize \textit{how} they communicate~\cite{pennebaker2022liwc}.

    \item \textbf{Descriptive metrics:} Finally, LIWC provides additional descriptive measures like total word count, average words per sentence, proportion of dictionary words, and the percentage of “big words” (words longer than seven letters). These serve as auxiliary indicators of text complexity and lexical richness.
\end{itemize}

Consequently,  LIWC's multidimensional approach enables measurable assessment of the psychological and social dimensions hidden inside of texts, making it a strong tool across psychology, social science, computer science, and digital communication research~\cite{pennebaker2022liwc}.

%geändert

\subsection{Psychometric Profiling with LIWC}

LIWC has become a central tool of psychological and behavioral language research due to its ability to extract psychologically meaningful dimensions from text. As Tausczik and Pennebaker~\cite{tausczik2010psychological} underline, LIWC bridges the gap between language and psychology by mapping linguistic usage onto cognitive, emotional, and social features, enabling the quantification and interpretation of even subtle psychological processes on a large scale. This capability makes LIWC essential across various fields. For example Fornaciari at. al.~\cite{fornaciari2013automatic} successfully applied LIWC to distinguish between truthful and fabricated statements in forensic transcripts, identifying linguistic markers of deception. Also, a recent study by Glasauer and Alexandrowicz~\cite{glasauer2022bigfive} illustrated the utility of LIWC for objective personality assessment. Using expressive writing samples from 124 participants, they modeled Big Five traits as latent constructs predicted by LIWC categories. While correlations with traditional self-reports were moderate, the approach showed promising model fits and validity. Similarly Farnadi et al.~\cite{farnadi2018user} demonstrated the effectiveness of LIWC for personality recognition in a large-scale multimodal setting. They extracted 88 LIWC-based features from Facebook status updates and compared them to alternative text representations like n-grams and word embeddings. LIWC features consistently outperformed these linguistic cues and were therefore used as the primary textual representation in their deep neural network models. This allowed the authors to capture psycholinguistically grounded indcators of personality particularly for the Big Five traits, while integrating them with visual and relational data. The relevance of LIWC has further increased through its integration into modern machine learning pipelines. Recent work by Kilic et al.~\cite{yakut-kilic-pan-2022-incorporating} demonstrates, that LIWC features enhance the predictive performance of neural networks and also provide insights into the linguistic markers of underlying behavioral predictions.

Therefore, LIWC serves as a theoretical and practical bridge between classical psycholinguistic analysis and the current state of computational modeling. 

%%geändert

\section{LIWC in Cybergrooming Research}
 As highlighted in the review by An et al.~\cite{an2025cybergrooming}, LIWC is among the most utilized psycholinguistic tools in computational cybergrooming detection. 
 A central application of LIWC lies in the linguistic characterization of grooming stages in chat-based conversations between adults and minors. Gupta et al.~\cite{gupta2012characterizingpedophileconversationsinternet} were among the first to segment annotated pedophile chat logs based on grooming theory and to use LIWC for creating psycholinguistic profiles of individual grooming phases. Their results showed that certain LIWC categories like \textit{social processes}, \textit{family}, or \textit{sexual} terms exhibit distinctive frequency patterns along the grooming timeline. Cano et al.~\cite{Cano2014} confirmed these findings by demonstrating, that integrating LIWC derived features into machine learning models improves the detection of grooming activities and phases, particularly in comparison to simple lexical baselines. %%% approved

More recent studies have extended the application of LIWC beyond offender profiling. Guo et al.~\cite{guo2023text} analysed psychological vulnerability markers of potential victims and quantified them using LIWC categories. %wichtig
 They found that personality traits and social support dimensions derived from LIWC distinguish between more and less vulnerable minors.  %wichtig ende 
 Another empirical study by Broome et al.~\cite{broome2020psycholinguistic} involved focus groups with police and correctional officers to identify relevant LIWC categories (for example affective, social, cognitive, biological processes), whose salience was then validated through the analysis of authentic grooming chats. The findings showed that grooming discourse is often not just characterized by overtly sexual content but also by frequent social bonding signals and a focus on the present. %%approved

The review by An et al.~\cite{an2025cybergrooming} further corroborates the high relevance of LIWC in grooming research. The authors highlight three main advantages:
\begin{itemize}
    \item LIWC provides interpretable and theory-driven language features that can be integrated into scalable machine learning models,
    \item It supports phase detection and offender profiling through the mapping of conversational structure,
    \item It enables cross-platform and cross-linguistic analysis due its validated and standardized category framework.
\end{itemize} %%approved

An et al.~\cite{an2025cybergrooming} further emphasize that LIWC categories like affective states, social processes, biological cues, and cognitive markers are not arbitrarily selected but have been empirically validated through repeated application in leading computational and social science studies. %%approved

%geändert

\subsection{Key psychometric LIWC Features in Grooming} \label{psychometric_liwc_features_in_grooming}
Especially the following LIWC dimensions have been proven to be most informative in the context of cybergrooming detection and linguistic analysis\cite{gupta2012characterizingpedophileconversationsinternet,broome2020psycholinguistic,an2025cybergrooming}):
\begin{itemize}
    \item \textbf{Affective processes}: positive/negative emotion, sadness, anxiety
    \item \textbf{Social processes}: family, friends, communication, group references
    \item \textbf{Cognitive processes}: insight, certainty, tentative language, causation
    \item \textbf{Biological processes}: body, sexual keywords
    \item \textbf{Drives and informal language}: risk, reward, netspeak, swear words
\end{itemize} %%approved

But it is important to note, that the LIWC categories can vary depending on the specific grooming stage~\cite{Cano2014} and the individual vulnerability factors are not uniformly distributed across all stages of grooming, but instead reflect the evolving tactics and psychological strategies of offenders. Still, these findings show, that LIWC offers a validated and interpretable set of psycholinguistic features in cybergrooming research~\cite{an2025cybergrooming, gupta2012characterizingpedophileconversationsinternet, Cano2014, guo2023text,broome2020psycholinguistic}. 
%%geändert  
\section{Explainable AI}

While deep learning models achieve remarkable performance in natural language processing tasks, their decision-making processes often remain hard to interpret. The field of Explainable AI (XAI) aims to address this by developing methods that make model behavior interpretable and comprehensible to human users.  One of the key techniques in XAI is the use of feature attribution methods, which assign importance scores to individual input features based on their contribution to the model's output. This allows users to understand which aspects of the input data most strongly influence the model's predictions. Two widely used methods for this purpose are SHAP (Shapley Additive explanations)~\cite{lundberg2017shap} and LIME (Local Interpretable Model-agnostic Explanations)~\cite{ribeiro2016lime}. Both approaches aim to explain individual model predictions by quantifying the importance of input features, but they differ in their underlying methodology and scope. Since in this thesis SHAP is used, the following section focuses on explaining this method in more detail. 
%geändert


\section{SHAP (Shapley Additive Explanations)}
\textbf{SHAP} was developed by Lundberg and Lee~\cite{lundberg2017shap} and is grounded in cooperative game theory and builds on the concept of \emph{Shapley values}. It treats the prediction task as a game in which each input feature contributes to the final outcome. \textbf{For a given model and instance, SHAP calculates how the model's prediction changes when a specific feature is added or removed from all possible subsets of features.} The resulting Shapley value of a feature is its average marginal contribution across all possible feature combinations. This makes SHAP a mathematically sound and globally consistent attribution method~\cite{lundberg2017shap}.

Formally, the SHAP value $\phi_i$ of a feature $i$ is defined as:

\[
\phi_i(f, x) = 
\sum_{S \subseteq N \setminus \{i\}} 
\frac{|S|!\,(|N|-|S|-1)!}{|N|!} 
\Big[ f_{S \cup \{i\}}(x_{S \cup \{i\}}) - f_S(x_S) \Big],
\]

where $N$ is the set of all features, $S$ is a subset of $N$ excluding $i$, and 
$f_S(x_S)$ denotes the model prediction based only on the features in $S$ 
\cite{lundberg2017shap}.

One of SHAP's core strengths is the satisfaction of desirable properties, which build on the classical axiomatization of Shapley values. The original Shapley value was characterized by the four axioms \textbf{Efficiency}, \textbf{Symmetry}, \textbf{Dummy Player Property}, and \textbf{Additivity}~\cite{shapley1953value}. SHAP reformulates these properties for the context of machine learning explanations and satisfies the following three key properties~\cite{lundberg2017shap}:

\begin{itemize}
    \item \textbf{Local Accuracy}: The sum of all SHAP values plus a base value (the expected model output) exactly equals the model prediction for the instance. This corresponds to the classical efficiency axiom~\cite{lundberg2017shap}.
    \item \textbf{Missingness}: Features that are missing or have no impact on the model output receive a SHAP value of zero. This property ensures unique solutions and extends the classical dummy player axiom~\cite{lundberg2017shap}.
    \item \textbf{Consistency}: If a model is changed such that a feature's marginal contribution increases (or stays the same) for all possible feature coalitions, its SHAP value does not decrease. This property ensures faithful representation of feature importance changes~\cite{lundberg2017shap}.
\end{itemize}

SHAP can be applied to any machine learning model, including complex deep learning architectures, making it a versatile tool for interpreting model behavior across different domains. \textbf{However, calculating exact Shapley values can be computationally expensive, especially for models with many features~\cite{rozemberczki2022shapley,aas2021explaining}.}  To address the computational complexity of SHAP value calculation, \textbf{KernelSHAP} was developed as an approximation method~\cite{lundberg2017shap}. KernelSHAP formulates the Shapley value computation as a weighted least squares regression problem which uses a sampling approach to approximate the high number of feature coalitions~\cite{covert2021}. As an alternative of evaluating all $2^p$ possible feature subsets (where $p$ is the number of features), KernelSHAP samples a smaller subset of coalitions and uses weighted regression to estimate the Shapley values, reducing computational complexity from exponential to polynomial time~\cite{rozemberczki2022shapley}. This makes SHAP applicable to high-dimensional models while maintaining computational costs.

Also, it is important to note that SHAP explanations can be affected by class imbalance. In strongly imbalanced datasets, the attributions may be dominated by the majority class, potentially obscuring signals from the minority class. Therefore balancing the data or carefully designing the background distribution is recommended to ensure faithful and interpretable explanations~\cite{liu2022balancedbackgroundexplanationdata,chen2024interpretable}.

%geändert

\section{Integrating Psycholinguistic Features and Explainable AI for Enhanced Grooming Detection}
Previous approaches to detect cybergrooming have mainly focused on linguistic features for the early identification of inappropriate communication~\cite{yakut-kilic-pan-2022-incorporating}, but have neglected the integration of validated psychological frameworks into explainable AI approaches~\cite{broome2020psycholinguistic}. Recent research demonstrates, that combining LIWC features with Transformer-based models and SHAP could enhance the interpretability and predictive performance of detection models.

Preiss and Chen~\cite{preiss-chen-2024-incorporating} show that integrating LIWC-22 into a Mental-RoBERTa classifier, combined with SHAP analyses, enables an extraction of psycholinguistically relevant text segments. Similar findings are reported by Wewelwala et al.~\cite{wewelwala2025hybrid} in clinical emotion analysis, where combining ClinicalBERT with LIWC, NRC and SHAP yielded complementary contributions, with the Transformer providing around 68\% of the predictive signal and LIWC and NRC contributing the remaining 32\%. 
Miah et al.~\cite{miah-etal-2011-detection} and Salminen et al.~\cite{salminen2025} extend these findings in a related, but for this work particularly relevant context, with the detection of toxic and harmful online communication. Both studies show, that combining contextual embeddings by BERT with LIWC based psycholinguistic features leads to more robust predictions and clearer, psychologically grounded explanations even in complex, multimodal chat data.

Despite these advances, a research gap remains in the field of cybergrooming detection. The fusion of LIWC features with modern language models and their evaluation using SHAP has not yet been implemented.

\textbf{This work addresses this gap by combining Transformer-based text representations of BERT with LIWC features and applying SHAP to quantify the contribution of both feature sets to model predictions. The aim is to improve the detection performance of cybergrooming, while enhancing explainability through psychologically grounded and transparent model interpretations, establishing a methodological foundation for future applications.}


%% geändert





