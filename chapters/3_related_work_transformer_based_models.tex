\section{Transformer Models for Text Classification}

Transformer models were first introduced by Vaswani et al.~\cite{vaswani2017attention} in their paper \textit{Attention Is All You Need}. Thanks to their design, Transformers are both effective for a wide range of NLP tasks and computationally efficient to train. Their success has led to a lot of variants, such as XML~\cite{lample2019cross}, GPT~\cite{radford2018gpt}, and XLNet~\cite{yang2019xlnet}. The central characteristic of Transformer Models lies in their reliance entirely on an \textbf{attention mechanisms}, enabling greater parallelization, more effective modeling of long-range dependencies, and a better contextual understanding~\cite{vaswani2017attention}.

Therefore, the core component is called the \textbf{self-attention} mechanism, which computes a weighted representation of each input token by attending to all other tokens in the sequence. Specifically, the model uses \textbf{scaled dot-product attention}, where the attention weights are calculated as follows:

\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]

Here, $Q$, $K$, and $V$ represent query, key, and value matrices derived from the input. This allows the model to determine \textbf{contextual relevance} across the entire sequence~\cite{vaswani2017attention}.

To enhance the model's capacity, the Transformer applies \textbf{multi-head attention}, which projects the input into multiple subspaces and performs attention in parallel. This enables the model to capture different types of relationships simultaneously~\cite{vaswani2017attention}.

Transformers furthermore use a stack of encoder and decoder layers, each composed of multi-head self-attention, feed-forward networks, and residual connections with layer normalization. Since no recurrence is used, positional encodings based on sinusoidal functions are added to the input embeddings to encode word order {vaswani2017attention}.

This allows each token to incorporate contextual information from the entire sequence, which results in rich internal representations. However, in sequence-to-sequence tasks like translation, the model additionally needs to connect the encoder outputs with the decoder states. This is achieved through \textbf{cross-attention}, which will be introduced in the following section~\cite{vaswani2017attention}.

%%geändert 

\subsection{Extension to cross-attention in transformer architectures.}
While self-attention enables a model to capture relationships in a single sequence, cross-attention extends this concept to the relations between different sources of information~\cite{vaswani2017attention}. \textbf{The key change here lies in the origin of the query, key, and value matrices:}

 In cross-attention, the queries (Q) originate from one information source, while keys (K) and values (V) are created from another source~\cite{vaswani2017attention}. This makes it possible, for example, to draw attention to text representations of additional features. The mathematical formulation remains unchanged:
\[
\mathrm{CrossAttention}\bigl(\mathbf{Q}_{\text{text}},\, \mathbf{K}_{\text{feature}},\, \mathbf{V}_{\text{feature}}\bigr)
= \operatorname{softmax}\!\left(\frac{1}{\sqrt{d_k}}\, \mathbf{Q}_{\text{text}} \mathbf{K}_{\text{feature}}^{\top}\right) \mathbf{V}_{\text{feature}}.
\]

The integration of additional information sources is often achieved using residual connections, allowing the model to decide, when and to what extent the additional information should influence the original representation~\cite{cai2025multimodal}. This flexibility makes cross-attention a strong tool for multimodal learning, where information from different modalities (for example text, images, audio) needs to be integrated to solve a specific task~\cite{li2024multimodal}. 

%%geändert 

\section{BERT}

BERT (Bidirectional Encoder Representations from Transformers), was first introduced by Devlin et al.~\cite{devlin2019bert} and is one of the most influential pre-trained language models, building on a \textbf{multi-layer bidirectional Transformer encoder.} Unlike unidirectional models like GPT~\cite{radford2018gpt}, which process text from left-to-right, BERT conditions on both left \textbf{and} right context, enabling it to capture deeper semantic relationships between words~\cite{rogers2020primerbertologyknowbert}.

To achieve this, BERT was pre-trained using \textbf{Masked Language Modeling} (MLM) and \textbf{Next Sentence Prediction} (NSP). In MLM, random tokens in a sentence are masked, so that the model learns to predict them based on the surrounding context. This prevents the model from only "seeing" the answer, enabling a deeper bidirectional context representation. In turn, NSP helps the model to understand inter-sentential relationships by training it to determine whether one sentence logically follows another~\cite{devlin2019bert}. This has been proven essential for a lot of downstream tasks like Natural Language Inference and Question Answering~\cite{sun2020finetuneberttextclassification}.

BERT was trained on an extensive document-level corpora, including BooksCorpus (800M words) and English Wikipedia (2.5B words) which avoided shuffled sentence-level datasets like the Billion Word Benchmark to preserve long-range dependencies~\cite{devlin2019bert}. Unlike earlier approaches that completely transferred word embeddings, BERT allows full parameter fine-tuning, making it highly adaptable for task-specific applications~\cite{korootev2021BERT}. This architectural design leads to better performance in diverse NLP tasks like text classification, sentiment analysis and false news detection. Studies have shown that BERT outperforms prior models even with negligible task adjustments. For example Quasim et al.~\cite{qasim2022fine} found, that BERT base and BERT large achieved modern accuracy across multiple real world datasets, highlighting its strength in classification applications.

In addition to BERT, many language models such as RoBERTa~\cite{liu2019roberta} and DeBERTa~\cite{he2021deberta} are based on the Transformer architecture and methodologically built on BERT. RoBERTa (“Robustly Optimized BERT Pretraining Approach”) was trained on a larger training corpus than BERT and, unlike BERT, doesn't use the next sentence prediction task, optimizing the masking procedure. As a result, RoBERTa achieves better results on various benchmarks such as GLUE, RACE and SQuAD and is particularly preferred for demanding classification tasks~\cite{liu2019roberta}. DeBERTa, on the other hand, integrates further innovations like disentangled attention and a redesigned masked decoding procedure, which further enhance the capacity and generalizability of the model~\cite{he2021deberta, he2023debertav3}.

%%geändert

\subsection{Fine-tuning BERT for classification tasks}
Fine-tuning BERT is a two-step process that adapts the pre-trained model to specific downstream tasks. In the first step, the entire BERT model is initialized with its pre-trained parameters, followed by an adjustment of all parameters using supervised learning on a labeled target dataset~\cite{devlin2019bert, sun2020finetuneberttextclassification}. This approach differs from feature extraction, where the BERT parameters remain frozen and only an aditional classification layer is trained~\cite{peters-fintune}. Studies show that fine-tuning is better than feature extraction in most cases because it allows the model to perfectly tailor its internal representation to the target task~\cite{peters-fintune, sun2020finetuneberttextclassification}.
BERT performs particularly well in binary classification tasks with balanced datasets due to its bidirectional context modeling.~\cite{bilal2022effectiveness}. 

The effectiveness of BERT in binary tasks is further enhanced by the fact, that the model develops specific linguistic information in different layers during the fine-tuning process. Peters et al.~\cite{peters-fintune} used mutual information analyses to show that in binary classification tasks, task-relevant knowledge is mainly concentrated in the upper layers of the model, while in more complex sentence pair tasks, information is built up across middle and upper layers~\cite{peters-fintune}. This explains why BERT performs exceptionally well in straightforward binary decisions.
In addition, recent studies show that BERT-like models remain outstanding in many binary classification tasks, even in the age of large language models, while requiring fewer computational resources than modern LLMs~\cite{zhang2025bert}. Nevertheless, it is important to note that the optimal configuration of the fine-tuning process, including consideration of the 512-token limit, appropriate learning rates, and batch sizes, is essential for achieving very good fine-tuning results~\cite{sun2020finetuneberttextclassification, bilal2022effectiveness}.
%%geändert

\section{Feature Fusion with BERT}
%%%%Überarbeitet

Feature fusion refers to the combination of multiple information sources to train a model. The idea behind this is that more robust representations can be generated than would be possible with a single source of information, since combined features are more meaningful than individual ones. The principle of feature fusion is one of the central principles of modern deep learning models, especially in the context of multimodal architectures~\cite{li2024multimodal}. 
Transformer models such as BERT are particularly suitable for integrating additional information like LIWC features, POS tags, or N-gram statistics due to their modular architecture and powerful context modeling~\cite{nagrani2021attention}.

\subsection{Classic fusion approaches}
A basic distinction of fusion approaches is made between three types of feature fusion, which differ mainly in the timing of integration~\cite{li2024multimodal, li2024fusionreview}:

\textbf{Early fusion:} Features from different sources are merged at the input level (for example, through concatenation or summation) and then processed together by a model. This allows interactions between modalities to be mapped early on, but is susceptible to noise in individual modalities~\cite{cai2025multimodal}.

\textbf{Late Fusion:} Here, the modalities are first processed separately and then combined at the decision level, for example, through weighted averaging, voting, or a special fusion layer.~\cite{sharma2023late}. In multimodal settings, late fusion also enables interpretability using SHAP. For example, the class-wise contribution of each unimodal predictor can be quantified and contrasted even at the cost of an additional fusion stage~\cite{abdul2024decoding}.

\textbf{Hybrid/Hierarchical Fusion:} This approach combines early and late fusion, allowing iterative fusion at different levels of the network. This allows the advantages of both strategies, but comes at the cost of higher costs and significantly greater architectural complexity, making interpretability more difficult~\cite{li2024multimodal}.

\textbf{Advanced work} further distinguishes between “early-to-mid fusion” (integration in lower to middle layers), “mid-to-late fusion” (integration after the first stage but before the end of the model), or “deep hierarchical fusion” (multiple integration across different layers) depending on the chosen integration point~\cite{li2024multimodal}.

\subsection{Cross-attention as a key mechanism}
A central trend in current research is the use of cross-attention to improve feature fusion. Cross-attention can be used in all fusion approaches (early/mid/hybrid) and allows flexible interaction between modalities.~\cite{nagrani2021attention,khan2020mmft}. 
   
In Cross-Attention, the integration is typically achieved with a gating system that adds the cross-attention output (H' = H + g \odot Attn(H, t)) in a way, that additional information is only incorporated if it is actually informative. This ensures that the model can fall back on the original BERT representation if the additional features are not helpful~\cite{cai2025multimodal}.

Overall, feature fusion further enhances the expressive power of BERT-like models and opens up new possibilities for applying transformers in specialized contexts, including the detection of complex behavioral patterns in cybergrooming.

%%geändert

\section{Transformer-based Methods for Cybergrooming Detection}

Transformer-based models have essentially transformed the field of automated cybergrooming detection by outperforming traditional machine learning approaches in the past years. The introduction of BERT and its subsequent improvements enabled semantic and syntactic modeling of chat conversations even in the presence of informality or unstructured grammar. Research shows that transformer-based models achieve high accuracy and robustness in detecting grooming behaviors in either early or late phases of chat-based dialogues~\cite{vogt2021early,street2024grooming,borj2022online,hamm2025llms}.

Despite this methodological progress, an important challenge remains. Most transformer models succeed at identifying \emph{what} is said in chat conversations, but base their decisions primarily on surface-level features like token frequencies, syntactic patterns, and keywords. Their \textit{black box nature} limits interpretability~\cite{mersha2025explainabilityneuralnetworksnatural,lokesh2025xai}. This limitation is especially apparent in sensitive domains such as medical or social text, where even fine-tuned BERT models often remain hard to interpret~\cite{talebi2024exploring}. In socially and psychologically  complex tasks like cybergrooming detection, where cues about trust-building, deception, manipulation, or progression across grooming phases are central for meaningful interpretation, the depth of psycholinguistic information remains underutilized. Broome et al.~\cite{broome2020psycholinguistic} and Street et al.~\cite{street2024grooming} highlight that discursive and psychological dynamics central to grooming are often marginalized in favor of corpus-driven features, while related work shows the potential of integrating psycholinguistic features into transformer architectures~\cite{maharjan2025llmliwc,zwanwar2022emotion,kerz2022transformers}.

Recent explainability frameworks for language models specifically address this gap not only clarifying \emph{which} linguistic units drive model decisions, but also identifying the psycholinguistic and emotional categories that influenced those predictions~\cite{maharjan2025llmliwc,zwanwar2022emotion,lokesh2025xai,ribeiro2024methodologyexplainablelargelanguage}.

This leads directly to the research focus of the present work. By combining transformer architectures with LIWC, there is a chance to strengthen the predictive power of transformer-based cybergrooming detection.

%%geändert

