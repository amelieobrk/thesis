\section{Transformer Models for Text Classification}

%%Überarbeitet

Transformer models were firstly introduced by Vaswani et al. \parencite{vaswani2017attention} in their paper \textit{Attention Is All You Need}. After being introduced, transformers became a key component in many state-of-the-art NLP models. Thanks to their design, Transformers are both highly effective for a wide range of NLP tasks and computationally efficient to train. Their success has led to a lot of variants, such as XML \parencite{lample2019cross}, GPT \parencite{radford2018gpt}, and XLNet\parencite{yang2019xlnet}. Thereby, the central characteristic of Transformer Models lies on their reliance, unlike previous sequence models based on recurrence or convolution, entirely on an \textbf{attention mechanisms}, enabling greater parallelization and more effective modeling of long range dependencies.

Therefore, the core component is called the \textbf{self-attention} mechanism, which computes a weighted representation of each input token by attending to all other tokens in the sequence. Specifically, the model uses \textbf{scaled dot-product attention}, where the attention weights are being calculated as:

\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]

Here, $Q$, $K$, and $V$ represent query, key, and value matrices derived from the input. This allows the model to dynamically determine \textbf{contextual relevance} across the entire sequence \parencite{vaswani2017attention}.

To enhance the model's capacity, the Transformer applies \textbf{multi-head attention}, which projects the input into multiple subspaces and performs attention in parallel. This enables the model to capture different types of relationships simultaneously \parencite{vaswani2017attention}.

Transformers use a stack of encoder and decoder layers, each composed of multi-head self-attention, feed-forward networks, and residual connections with layer normalization. Since no recurrence is used, positional encodings based on sinusoidal functions are added to the input embeddings to encode word order {vaswani2017attention}.

The Transformer architecture has been widely adopted in a lot of NLP tasks, including machine translation, text classification and question answering. Its ability to model long-range dependencies and its parallelizable structure have made it a foundational model for many subsequent advancements in the field.

\subsection{Extension to cross-attention in transformer architectures.}
While self-attention enables a model to capture relationships within a single sequence, cross-attention extends the concept to the interaction between different sources of information~\parencite{vaswani2017attention}. The key change here lies in the origin of the query, key, and value matrices. In cross-attention, the queries (Q) originate from one modality or information source, while keys (K) and values (V) are created from another source~\parencite{vaswani2017attention}. This makes it possible, for example, to draw the attention of text representations to additional features. The mathematical formulation remains unchanged:
\[
\mathrm{CrossAttention}\bigl(\mathbf{Q}_{\text{text}},\, \mathbf{K}_{\text{feature}},\, \mathbf{V}_{\text{feature}}\bigr)
= \operatorname{softmax}\!\left(\frac{1}{\sqrt{d_k}}\, \mathbf{Q}_{\text{text}} \mathbf{K}_{\text{feature}}^{\top}\right) \mathbf{V}_{\text{feature}}.
\]

Integration is often achieved via residual connections, allowing the model to dynamically decide when and to what extent the additional information should influence the original representation~\parencite{cai2025multimodal}. This flexibility makes cross-attention a powerful tool for multimodal learning, where information from different modalities (e.g., text, images, audio) needs to be integrated effectively~\parencite{li2024multimodal}. %% approved


%%%Vielleicht hier noch eine Transformer Architektur rein


\section{BERT}
%% Überarbeitet
BERT (Bidirectional Encoder Representations from Transformers), was first introduced by Devlin et al.~\textcite{devlin2019bert} and is one of the most influential pre-trained language models which builds upon a multi-layer bidirectional Transformer encoder. Unlike unidirectional models such as GPT~\parencite{radford2018gpt}, which process text left-to-right, BERT jointly conditions on both left and right context, enabling it to capture deeper semantic relationships between words~\parencite{rogers2020primerbertologyknowbert}.

To achieve this, BERT was pre-trained using two unsupervised objectives: \textbf{Masked Language Modeling} (MLM) and \textbf{Next Sentence Prediction} (NSP). In MLM, random tokens in a sentence are masked, so that the model learns to predict them based on the surrounding context~\parencite{devlin2019bert}. This prevents the model from only "seeing" the answer, enabling deep bidirectional context representations. NSP, in turn, helps the model understand inter-sentential relationships by training it to determine whether one sentence logically follows another. This has proven essential for a lot of downstream tasks such as Natural Language Inference (NLI) and Question Answering (QA)~\parencite{sun2020finetuneberttextclassification}.

BERT was trained on a large scale, document level corpora including BooksCorpus (800M words) and English Wikipedia (2.5B words), avoiding shuffled sentence level datasets like the Billion Word Benchmark to preserve long range dependencies~\parencite{sun2020finetuneberttextclassification}. Unlike earlier approaches that entirely transferred word embeddings, BERT allows full parameter fine-tuning, making it highly adaptable for task specific applications~\parencite{korootev2021BERT}. This architectural design leads to better performance in several NLP tasks such as text classification, sentiment analysis and false news detection. Studies have shown that BERT outperforms prior models even with negligible task specific adjustments. For example \textcite{qasim2022fine} found that BERT base and BERT large achieved modern accuracy across multiple real world datasets, highlighting its strength in classification based applications.

In addition to BERT, many language models such as RoBERTa \parencite{liu2019roberta} and DeBERTa \parencite{he2021deberta} are based on the Transformer architecture and build methodologically on BERT. RoBERTa (“Robustly Optimized BERT Pretraining Approach”) was trained on a larger training corpus than BERT and, unlike BERT, does not use the next sentence prediction task and optimizes the masking procedure. As a result, RoBERTa achieves better results on various benchmarks such as GLUE, RACE and SQuAD and is particularly preferred for demanding classification tasks~\parencite{liu2019roberta}. DeBERTa, on the other hand, integrates further innovations, such as disentangled attention and a redesigned masked decoding procedure, which further improve the model's capacity and generalizability~\parencite{he2021deberta, he2023debertav3}.


\textbf{Fine-tuning mechanism for classification tasks}
Fine-tuning BERT is a two-step process that adapts the pre-trained model to specific downstream tasks. In the first step, the entire BERT model is initialized with its pre-trained parameters, followed by an adjustment of all parameters using supervised learning on a labeled target dataset~\parencite{devlin2019bert, sun2020finetuneberttextclassification}. This approach differs fundamentally from feature extraction, where the BERT parameters remain frozen and only an additional classification layer is trained \cite{peters-fintune}. Studies show that fine-tuning is better than feature extraction in most cases because it allows the model to perfectly tailor its internal representation to the target task~\parencite{peters-fintune, sun2020finetuneberttextclassification}.
BERT performs particularly well in binary classification tasks with balanced datasets due to its bidirectional context modeling. \parencite{bilal2022effectiveness}. 

The effectiveness of BERT in binary tasks is further enhanced by the fact that the model develops specific linguistic information in different layers during the fine-tuning process. Peters et al. \cite{peters-fintune} used mutual information analyses to show that in binary classification tasks, task-relevant knowledge is mainly concentrated in the upper layers of the model, while in more complex sentence pair tasks, information is gradually built up across middle and upper layers~\parencite{peters-fintune}. This finding explains why BERT performs particularly well in straightforward binary decisions.
In addition, recent studies confirm that BERT-like models remain superior in many binary classification tasks, especially pattern-driven tasks, even in the age of large language models, while requiring fewer computational resources than modern LLMs~\parencite{zhang2025bert}. However, it is important to note that the optimal configuration of the fine-tuning process, including consideration of the 512-token limit, appropriate learning rates, and batch sizes, is crucial for achieving very good results~\parencite{sun2020finetuneberttextclassification, bilal2022effectiveness}.



\section{Feature Fusion with BERT}
%%%%Überarbeitet

Feature fusion refers to the combination of multiple information sources to train a model. The idea behind this is that more robust representations can be generated than would be possible with a single source of information, since combined features are more meaningful than individual ones. The principle of feature fusion is one of the central principles of modern deep learning models, especially in the context of multimodal architectures~\parencite{li2024multimodal}. 
Transformer models such as BERT are particularly well suited for integrating additional information such as LIWC features, POS tags, or N-gram statistics due to their modular architecture and powerful context modeling~\parencite{nagrani2021attention}.

\subsection{Classic fusion approaches}
A basic distinction is made between three types of feature fusion, which differ mainly in the timing of integration~\parencite{li2024multimodal, li2024fusionreview}:

\textbf{Early fusion:} Features from different sources are merged at the input level (e.g., through concatenation or summation) and then processed together by a model. This allows interactions between modalities to be mapped early on, but is susceptible to noise in individual modalities~\parencite{cai2025multimodal}.

\textbf{Late Fusion:} Here, the modalities are first processed separately and then combined at the decision level, for example, through weighted averaging, voting, or a special fusion layer.~\parencite{sharma2023late}. In multimodal settings, late fusion also enables interpretability using SHAP, for example the class-wise contribution of each uni-modal predictor can be quantified and contrasted even at the cost of an additional fusion stage~\parencite{abdul2024decoding}.

\textbf{Hybrid/Hierarchical Fusion:} This approach combines early and late fusion, allowing iterative fusion at different levels of the network. This allows the advantages of both strategies to be exploited, but comes at the cost of higher costs and significantly greater architectural complexity, making interpretability more difficult~\parencite{li2024multimodal}.

\textbf{Advanced work} further distinguishes between “early-to-mid fusion” (integration in lower to middle layers), “mid-to-late fusion” (integration after the first stage but before the end of the model), or “deep hierarchical fusion” (multiple integration across different layers) depending on the chosen integration point~\parencite{li2024multimodal}.

\subsection{Cross-attention as a key mechanism}
A central trend in current research is the use of cross-attention to improve feature fusion. Cross-attention can be used flexible in all fusion approaches (early/mid/hybrid) and allows flexible interaction between modalities.~\parencite{nagrani2021attention,khan2020mmft}. 

In the context of BERT, this means that the hidden states of the text representation can specifically pay attention to additional compressed features~\parencite{biggiogera2021,cai2025multimodal}. This creates a context-dependent interaction that weights additional information without disrupting the proven language modeling~\parencite{cai2025multimodal}.    
In Cross-Attention, the integration is typically achieved with a gating system that adds the cross-attention output (H' = H + g \odot Attn(H, t)) in a way that additional information is only incorporated if it is actually informative. This ensures that the model can fall back on the original BERT representation if the additional features are not helpful~\parencite{cai2025multimodal}.

\section{Transformer-based Methods for Cybergrooming Detection}

Transformer-based models have fundamentally transformed the field of automated cybergrooming detection, decidedly outperforming traditional machine learning approaches in recent years. The introduction of BERT (Bidirectional Encoder Representations from Transformers) and its subsequent improvement in RoBERTa (Robustly Optimized BERT Pretraining Approach) enabled robust semantic and syntactic modeling of chat conversations, even in the presence of informality or unstructured grammar, as frequently encountered in online communication. Research shows that BERT- and RoBERTa-based frameworks achieve high accuracy and robustness in detecting grooming behaviors, both in early and aggregate phases of chat dialogues~\parencite{vogt2021early}. 

In direct comparison, RoBERTa has been shown to outperform not only classical classifiers but also BERT itself in the context of online grooming detection~\parencite{street2024grooming}. A likely reason for this advantage is RoBERTa’s more extensive and diverse pretraining corpus, coupled with the removal of the Next Sentence Prediction (NSP) objective, which allows for richer contextual representations~\parencite{liu2019roberta}. %%approved

At present, DeBERTa (Decoding-enhanced BERT with Disentangled Attention) is considered among the leading architectures for nuanced text classification in social media contexts. Hassan et al. ~\parencite{hassan2023fircatsemeval} demonstrate that ensembles of fine-tuned DeBERTa models achieve state-of-the-art results on complex, fine-grained classification tasks involving harmful online content, such as online sexism. Their approach, which systematically compares DeBERTa against other popular transformer models including BERT, RoBERTa, and XLM-RoBERTa, consistently shows that DeBERTa delivers the best performance in terms of precision, recall, and F1-score across all levels of granularity in the shared task. %%% approved

A core limitation of transformer-based models such as BERT, RoBERTa, and DeBERTa is their maximum input length, typically 512 tokens. Chat conversations in the context of cybergrooming, however, often greatly exceed this limit. To address this, research such as Vogt et al.~\cite{vogt2021early} propose a sliding window approach: long chats are divided into overlapping segments, each of which is analyzed separately by the model, and the results are then aggregated for a robust prediction over the entire conversation. This technique enables transformers to process arbitrarily long dialogues and is now standard for grooming detection on real data streams. %%% approved

Despite this methodological progress, an important challenge remains. most transformer models excel at identifying \emph{what} is said in chat conversations but base their decisions primarily on surface-level features such as token frequencies, syntactic patterns, and keywords. Their ``black box'' nature limits interpretability. Psycholinguistic features,for example, cues related to trust building, deception, emotional manipulation, or the progression of grooming phases are rarely modeled explicitly. Broome \textit{et al.} and Street \textit{et al.} highlight that discursive and psychological dynamics central to grooming are often marginalized in favor of quantitative, corpus-driven features~\cite{broome2020psycholinguistic},~\cite{street2024grooming}. %%%approved
Therefore, a crucial challenge remains in the field of cybergrooming detection. While models such as BERT, RoBERTa, and DeBERTa reliably classify what is said in chats, their decisions are largely grounded in superficial patterns like token frequencies, syntactic structures, and specific keywords. As highlighted by Mersha et al.~\cite{mersha2025explainabilityneuralnetworksnatural} in their comprehensive analysis of explainability in neural NLP models, these transformers still lack genuine transparency—their inherent “black box” nature makes it difficult for users to understand or validate the basis of any given decision. This limitation is especially apparent in sensitive domains such as medical or social text, where even fine-tuned BERT models often remain hard to interpret~\cite{talebi2024exploring}. %%%approved

In socially and psychologically complex tasks such as cybergrooming detection, where cues about trust-building, deception, manipulation, or progression across grooming phases are central for meaningful interpretation—a significant depth of psycholinguistic information remains underutilized. Broome \textit{et al.} and Street \textit{et al.} show that relevant discursive and psychological dynamics of grooming are often lost within purely quantitative, corpus-driven features~\cite{broome2020psycholinguistic},~\cite{street2024grooming}. %%approved

Recent explainability frameworks for language models specifically address this gap: Ribeiro et al.~\cite{ribeiro2024methodologyexplainablelargelanguage} present a methodology that combines techniques such as Integrated Gradients with psycholinguistic tools like LIWC. This hybrid approach not only clarifies \emph{which} linguistic units drive model decisions, but also identifies the crucial psycholinguistic and emotional categories—such as trust markers and affectivity—that influenced those predictions. Methods such as SHAP or the explicit integration of LIWC attributions can thus substantially improve both model transparency and the psychological interpretability of transformer-based predictions. %%%approved

This leads directly to the research focus of the present work. By combining transformer architectures with LIWC and explainability frameworks such as SHAP, there is a chance to strengthen the predictive power and the interpretative depth of online grooming detection systems.
